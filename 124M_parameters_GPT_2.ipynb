{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "GtqmzevhyX_o"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "SsfJVPl9o88e"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    'vocab_size':50257,\n",
        "    'context_length':1024,\n",
        "    'emb_dim' : 768,\n",
        "    'n_head':12,\n",
        "    'n_layers':12,\n",
        "    'drop_rate':0.1,\n",
        "    'qkv_bias':False\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer_Norm(nn.Module):\n",
        "  def __init__(self,emb_dim):\n",
        "    super().__init__()\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.eps = 1e-5\n",
        "\n",
        "  def forward(self,x):\n",
        "    mean = x.mean(dim=-1,keepdim=True)\n",
        "    var = x.var(dim=-1,keepdim=True, unbiased=True)\n",
        "    return self.scale*(x-mean)/torch.sqrt(var+self.eps) + self.shift\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self,x):\n",
        "    return 0.5*x*(1+torch.tanh(torch.tensor((2/torch.pi))**0.5)*(x+0.044715*x**3))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "        super().__init__()\n",
        "        self.layers =  nn.Sequential(\n",
        "        nn.Linear(cfg['emb_dim'],4*cfg['emb_dim']),    #expansion\n",
        "        GELU(),   #non linear activation\n",
        "        nn.Linear(4*cfg['emb_dim'],cfg['emb_dim'])  #compression\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "eTYdHjiWyVuj"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self,din,dout,context_length,dropout,num_heads,qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.w_queries = nn.Linear(din,dout,qkv_bias)\n",
        "    self.w_keys = nn.Linear(din,dout,qkv_bias)\n",
        "    self.w_values = nn.Linear(din,dout,qkv_bias)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    self.context_length=context_length\n",
        "    self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length)))\n",
        "    self.num_heads=num_heads\n",
        "    self.out_proj = nn.Linear(dout,dout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    b,contextlength, emd_size = x.shape\n",
        "    num_heads=self.num_heads\n",
        "    head_dim = emd_size//num_heads\n",
        "\n",
        "    queries = self.w_queries(x)\n",
        "    keys = self.w_keys(x)\n",
        "    values = self.w_values(x)\n",
        "\n",
        "    queries = queries.view(b,contextlength,num_heads,head_dim)\n",
        "    keys = queries.view(b,contextlength,num_heads,head_dim)\n",
        "    values = queries.view(b,contextlength,num_heads,head_dim)\n",
        "\n",
        "    queries = queries.transpose(1,2)\n",
        "    keys = keys.transpose(1,2)\n",
        "    values = values.transpose(1,2)\n",
        "\n",
        "    attention_scores = queries @ keys.transpose(2,3)\n",
        "    attention_scores.masked_fill(self.mask.bool()[:contextlength, :contextlength],-torch.inf)\n",
        "    attention_weights = torch.softmax(attention_scores/num_heads**0.5,dim=-1)\n",
        "    attention_weights = self.dropout(attention_weights)\n",
        "    context_vectors = (attention_weights @ values).transpose(1,2)\n",
        "    context_vectors = context_vectors.contiguous().view(b,contextlength,emd_size)\n",
        "    context_vectors = self.out_proj(context_vectors)\n",
        "    return context_vectors\n"
      ],
      "metadata": {
        "id": "AdGm8WDcybSC"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self,cfg):\n",
        "      super().__init__()\n",
        "      self.norm1 = Layer_Norm(cfg['emb_dim'])\n",
        "      self.norm2 = Layer_Norm(cfg['emb_dim'])\n",
        "      self.att = MultiheadAttention(din=cfg['emb_dim'],\n",
        "                                    dout=cfg['emb_dim'],\n",
        "                                    context_length=cfg['context_length'],\n",
        "                                    dropout=cfg['drop_rate'],\n",
        "                                    num_heads=cfg['n_head'])\n",
        "      self.ff = FeedForward(cfg)\n",
        "      self.drop_shortcut = nn.Dropout(cfg['drop_rate'])\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "      shortcut = x\n",
        "      x = self.norm1(x)\n",
        "      x = self.att(x)\n",
        "      x = self.drop_shortcut(x)\n",
        "      x = x+shortcut\n",
        "      shortcut = x\n",
        "      x = self.norm2(x)\n",
        "      x = self.ff(x)\n",
        "      x = self.drop_shortcut(x)\n",
        "      x = x+shortcut\n",
        "\n",
        "      return x\n"
      ],
      "metadata": {
        "id": "T_EjguwByb3m"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GPTModel(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "      super().__init__()\n",
        "      self.tok_embedding = nn.Embedding(cfg['vocab_size'],cfg['emb_dim'])\n",
        "      self.pos_embedding = nn.Embedding(cfg['context_length'],cfg['emb_dim'])\n",
        "      self.drop_emb = nn.Dropout(cfg['drop_rate'])\n",
        "      self.trf_blocks = nn.Sequential(\n",
        "          *[Transformer(cfg) for _ in range(cfg['n_layers'])]\n",
        "      )\n",
        "      self.final_norm = Layer_Norm(cfg['emb_dim'])\n",
        "      self.out_head = nn.Linear(cfg['emb_dim'],cfg['vocab_size'],bias=False)\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "      batch_size, seq_len = x.shape\n",
        "      tok_embeds = self.tok_embedding(x)\n",
        "      pos_embeds = self.pos_embedding(torch.arange(seq_len,device=x.device))\n",
        "      x = tok_embeds + pos_embeds\n",
        "      x = self.drop_emb(x)\n",
        "      x = self.trf_blocks(x)\n",
        "      x = self.final_norm(x)\n",
        "      logits = self.out_head(x)\n",
        "      return logits"
      ],
      "metadata": {
        "id": "GbwsCYIYpq9r"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "x = torch.tensor([[6109,3626,6100,345],[6109,1110,6622,257]])\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "op = model.forward(x)\n",
        "print(op.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uaqt4HCNytqC",
        "outputId": "ed1dfa4c-f6e9-4d25-81ef-9cd72e737a71"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 4, 50257])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(op)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSokRQsWzCtk",
        "outputId": "6f0e7031-8c2f-479c-d85b-bcf85ae8559d"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.6212, -1.0549, -0.7801,  ..., -0.1406, -1.4150,  0.3253],\n",
            "         [ 0.5819, -1.4818,  0.0543,  ..., -0.7155, -1.3151,  0.1606],\n",
            "         [-0.4098, -0.8797, -0.1308,  ..., -0.6430, -1.2205,  0.2275],\n",
            "         [-0.4459, -0.5298, -0.5528,  ...,  0.7475, -0.2636, -0.2226]],\n",
            "\n",
            "        [[-0.6213, -0.7357, -0.4994,  ..., -0.4088, -1.1084,  0.4430],\n",
            "         [ 0.7592, -1.3652,  0.7986,  ...,  0.3807, -1.1525,  0.4100],\n",
            "         [-0.1499, -1.1212, -0.5046,  ..., -0.5478, -0.8977,  0.4323],\n",
            "         [-0.5898, -1.3921,  0.1194,  ...,  0.6098, -1.3501,  0.8240]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"{total_params:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SmX_rBtz5Lk",
        "outputId": "0fced12b-ffa7-4302-be02-411242c1329a"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163,009,536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.out_head.weight.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cK3JhFC0fIz",
        "outputId": "c093eeb9-8aab-46a9-ef23-7c4646e719b5"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50257, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.tok_embedding.weight.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqPHKdTM16qV",
        "outputId": "47a72a7e-49fe-486c-aed4-4f186cbf68aa"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50257, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reuse the tok_emb parameters for the out_head parameters"
      ],
      "metadata": {
        "id": "_ft0ckWG11hM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_total_params = total_params - sum(p.numel() for p in model.out_head.parameters())\n",
        "print(f\"{final_total_params:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfsA-htc1gUk",
        "outputId": "90ed8204-26e7-4f32-a0e8-c6a62359117e"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "124,412,160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_size_bytes = total_params *4\n",
        "total_size_mb = total_size_bytes/(1024*1024)\n",
        "print(f\"{total_size_mb:,} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxJ066NG2TNQ",
        "outputId": "64777d0f-2eac-4554-8582-b60cc7511c56"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "621.83203125 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_simple(model,idx,max_new_tokens,context_length):\n",
        "  for _ in range(max_new_tokens):\n",
        "    idx_cond = idx[:,-context_length:]\n",
        "    with torch.no_grad():\n",
        "      logits = model(idx_cond)\n",
        "    logits = logits[:,-1,:]\n",
        "    probas = torch.softmax(logits,dim=-1)\n",
        "    idx_next = torch.argmax(probas,dim=-1,keepdim=True)\n",
        "    idx= torch.cat((idx,idx_next),dim=1)\n",
        "\n",
        "  return idx\n"
      ],
      "metadata": {
        "id": "kRnEJ7aL2z5x"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken"
      ],
      "metadata": {
        "id": "GLZrlqUZwIZg"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding('gpt2')"
      ],
      "metadata": {
        "id": "UGTF0r-5wvmm"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_context =\"Hello, I am\"\n",
        "encoded = tokenizer.encode(start_context)\n",
        "print(encoded)\n",
        "encoded = torch.tensor(encoded).unsqueeze(0)\n",
        "# model.eval()\n",
        "op=generate_text_simple(model,encoded,6,GPT_CONFIG_124M['context_length'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIjwjUKCv6Uf",
        "outputId": "9f121f8c-88da-4939-ef7b-a5ffb2754469"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 314, 716]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(op.squeeze(0).tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kwkpKn1yyAJt",
        "outputId": "4323e0b3-a56a-4e8b-c3b6-028b66bf0499"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello, I am39 proliferation proliferation proliferation nation assuming'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "169cSpfe1ctZ"
      },
      "execution_count": 133,
      "outputs": []
    }
  ]
}