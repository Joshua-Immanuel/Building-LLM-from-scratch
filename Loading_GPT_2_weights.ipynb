{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4x9GFM0xGOky"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2cvua0tyd8n",
        "outputId": "0648ae7d-f3f2-47c6-e9b7-d3cd6a3adc82"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding('gpt2')"
      ],
      "metadata": {
        "id": "sAs8Kx0wlGlq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "kI4GfLULlMps"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GPT2_124M_CFG={\n",
        "    'dropout':0.1,\n",
        "    'n_layers':12,\n",
        "    'n_heads':12,\n",
        "    'emb_size':768,\n",
        "    'context_length':1024,\n",
        "    'vocab_size':50257,\n",
        "    'qkv_bias':False\n",
        "}"
      ],
      "metadata": {
        "id": "V_XuWosyia1D"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class LayerNormalization(nn.Module):\n",
        "  def __init__(self,emb_size):\n",
        "       super().__init__()\n",
        "       self.scale = nn.Parameter(torch.ones(emb_size))\n",
        "       self.shift = nn.Parameter(torch.ones(emb_size))\n",
        "       self.eps = 1e-5\n",
        "\n",
        "  def forward(self,x):\n",
        "      mean = torch.mean(x,dim=-1,keepdim=True)\n",
        "      var = torch.var(x,dim=-1,keepdim=True,unbiased=False)\n",
        "      return self.scale* (x-mean)/torch.sqrt(var+self.eps) + self.shift\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(cfg['emb_size'],4*cfg['emb_size']),\n",
        "        GELU(),\n",
        "        nn.Linear(4*cfg['emb_size'],cfg['emb_size'])\n",
        "    )\n",
        "  def forward(self,x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "JDvcGvxHGiT2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self,din,dout,n_heads,context_length,dropout,qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.w_queries = nn.Linear(din,dout,qkv_bias)\n",
        "        self.w_keys = nn.Linear(din,dout,qkv_bias)\n",
        "        self.w_values = nn.Linear(din,dout,qkv_bias)\n",
        "        self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
        "        self.out_layer = nn.Linear(dout,dout)\n",
        "        self.n_heads= n_heads\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    batch, context_length, emb_size = x.shape\n",
        "    queries = self.w_queries(x)\n",
        "    keys = self.w_keys(x)\n",
        "    values = self.w_values(x)\n",
        "    head_dim = emb_size//self.n_heads\n",
        "\n",
        "    queries = queries.view(batch,context_length,self.n_heads,head_dim)\n",
        "    keys = keys.view(batch,context_length,self.n_heads,head_dim)\n",
        "    values = values.view(batch,context_length,self.n_heads,head_dim)\n",
        "     # b,cl,nheads,hd.\n",
        "\n",
        "\n",
        "     # b, nheads, cl, hd\n",
        "    queries = queries.transpose(1,2)\n",
        "    keys = keys.transpose(1,2)\n",
        "    values = values.transpose(1,2)\n",
        "\n",
        "    attention_scores = queries @ keys.transpose(2,3)\n",
        "       #b, nheads, cl,cl\n",
        "\n",
        "    attention_scores.masked_fill_(self.mask.bool()[:context_length,:context_length],-torch.inf)\n",
        "\n",
        "    attention_weights = torch.softmax(attention_scores/(keys.shape[-1])**0.5, dim =-1)\n",
        "    attention_weights = self.dropout(attention_weights)\n",
        "    context_vectors = (attention_weights @ values).transpose(1,2)\n",
        "    # b, nheads,cl,cl.     b,nheads, cl, hd\n",
        "    # b, nheads, cl,hd.     .T -> b, cl,nheads,hd\n",
        "    context_vectors = context_vectors.contiguous().view(batch, context_length, emb_size)\n",
        "    context_vectors = self.out_layer(context_vectors)\n",
        "    return context_vectors\n"
      ],
      "metadata": {
        "id": "8DnJu2gKacdu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.ff = FeedForward(cfg)\n",
        "    self.dropout = nn.Dropout(cfg['dropout'])\n",
        "    self.norm1 = LayerNormalization(cfg['emb_size'])\n",
        "    self.norm2 = LayerNormalization(cfg['emb_size'])\n",
        "    self.att = MultiheadAttention(\n",
        "        din = cfg['emb_size'],\n",
        "        dout = cfg['emb_size'],\n",
        "        n_heads=cfg['n_heads'],\n",
        "        context_length = cfg['context_length'],\n",
        "        dropout = cfg['dropout'],\n",
        "        qkv_bias=cfg['qkv_bias'])\n",
        "\n",
        "  def forward(self,x):\n",
        "    shortcut = x\n",
        "    x = self.norm1(x)\n",
        "    x= self.att(x)\n",
        "    x= self.dropout(x)\n",
        "    x= shortcut+x\n",
        "\n",
        "    shortcut = x\n",
        "    x = self.norm2(x)\n",
        "    x= self.ff(x)\n",
        "    x= self.dropout(x)\n",
        "    x= shortcut+x\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "ibjRx_PtgRFu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb = nn.Embedding(cfg['vocab_size'],cfg['emb_size'])\n",
        "    self.pos_emb = nn.Embedding(cfg['context_length'],cfg['emb_size'])\n",
        "    self.out_head = nn.Linear(cfg['emb_size'],cfg['vocab_size'],bias=False)\n",
        "    self.drop_emb = nn.Dropout(cfg['dropout'])\n",
        "    self.blocks = nn.Sequential(\n",
        "        *[Transformer(cfg) for _ in range(cfg['n_layers'])]\n",
        "    )\n",
        "    self.final_norm = LayerNormalization(cfg['emb_size'])\n",
        "\n",
        "  def forward(self,x):\n",
        "    batch, n_tokens = x.shape\n",
        "    token_embed = self.tok_emb(x)\n",
        "    position_embed = self.pos_emb(torch.arange(n_tokens,device=x.device))\n",
        "    x =  token_embed + position_embed\n",
        "    x= self.drop_emb(x)\n",
        "    x= self.blocks(x)\n",
        "    x= self.final_norm(x)\n",
        "    logits= self.out_head(x)\n",
        "    return logits\n"
      ],
      "metadata": {
        "id": "ccagkS76jFSs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_simple(idx,model,max_words,context_length):\n",
        "  model.eval()\n",
        "  for _ in range(max_words):\n",
        "   idx_new = idx[:,-context_length:]\n",
        "   with torch.no_grad():\n",
        "    logits = model(idx_new)\n",
        "   temp = logits[:,-1,:]\n",
        "   temp= torch.softmax(temp,dim=-1)\n",
        "   next = torch.argmax(temp,dim=-1,keepdim=True)\n",
        "   idx = torch.cat((idx,next),dim=-1)\n",
        "  return idx"
      ],
      "metadata": {
        "id": "x2daSTKksiMZ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0)\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "start_context = \"Every effort moves you\""
      ],
      "metadata": {
        "id": "oXj3N_NnWy6D"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(idx,context_length,model,max_words,temperature=0.0,top_k=None,end_token=None):\n",
        "  model.eval()\n",
        "  for _ in range(max_words):\n",
        "      new_idx = idx[:,-context_length:]\n",
        "      with torch.no_grad():\n",
        "        logits = model(new_idx)\n",
        "      logits = logits[:,-1,:]\n",
        "\n",
        "     # temperature scaling + topk sampling\n",
        "      if top_k is not None:\n",
        "          top_logits,_ = torch.topk(logits,top_k)\n",
        "          min_val = top_logits[:,-1].unsqueeze(-1)\n",
        "          logits = torch.where(logits<min_val,torch.tensor(float(\"-inf\")).to(logits.device),logits)\n",
        "      if temperature>0.0:\n",
        "          logits = logits/temperature\n",
        "          probas = torch.softmax(logits,dim=-1)\n",
        "          next = torch.multinomial(probas,num_samples=1)\n",
        "      else:\n",
        "        logits = torch.softmax(logits,dim=-1)\n",
        "        next = torch.argmax(logits,dim=-1,keepdim=True)\n",
        "      if next==end_token:\n",
        "          break\n",
        "      idx = torch.cat((idx,next),dim=-1)\n",
        "  return idx\n"
      ],
      "metadata": {
        "id": "pB3uhRHYgOIo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tqdm\n",
        "\n",
        "print(\"tensorflow version\",tf.__version__)\n",
        "print(\"tqdm verison\",tqdm.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcpVLzBalYv6",
        "outputId": "479e8416-9468-4995-80e6-21e4782d98c5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensorflow version 2.18.0\n",
            "tqdm verison 4.67.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt_download3 import download_and_load_gpt2"
      ],
      "metadata": {
        "id": "y-CtjCpxC_Oa"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "setting, params = download_and_load_gpt2(model_size='124M',models_dir=\"gpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Yh3-5jlDHLG",
        "outputId": "9abd6b73-ccc3-4f4f-9d61-325a4906d7b3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 96.8kiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "encoder.json: 100%|██████████| 1.04M/1.04M [00:01<00:00, 955kiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 139kiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [01:45<00:00, 4.70MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 6.89MiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 601kiB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n",
            "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 581kiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NEW_CONFIG = GPT2_124M_CFG\n",
        "NEW_CONFIG.update({'context_length':1024,'qkv_bias':True})"
      ],
      "metadata": {
        "id": "EgTua4QpELlt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NEW_CONFIG"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hTXkQi3Uv5e",
        "outputId": "10898e5a-ca32-4abe-baa1-344d02ea26fc"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dropout': 0.1,\n",
              " 'n_layers': 12,\n",
              " 'n_heads': 12,\n",
              " 'emb_size': 768,\n",
              " 'context_length': 1024,\n",
              " 'vocab_size': 50257,\n",
              " 'qkv_bias': True}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt = GPT2(cfg=NEW_CONFIG)\n",
        "gpt.eval();"
      ],
      "metadata": {
        "id": "TMkiCjuQFhJf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assign(left,right):\n",
        "  if left.shape != right.shape:\n",
        "    raise ValueError(\"shape mismatch\")\n",
        "  return torch.nn.Parameter(torch.tensor(right))"
      ],
      "metadata": {
        "id": "jdhdzSwRJu3V"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def load_weights_into_gpt(gpt,params):\n",
        "  gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
        "  gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
        "\n",
        "  for b in range(len(params['blocks'])):\n",
        "\n",
        "        q_w, k_w, v_w = np.split(params['blocks'][b]['attn']['c_attn']['w'],3,axis=-1)\n",
        "        gpt.blocks[b].att.w_queries.weight = assign(gpt.blocks[b].att.w_queries.weight,q_w.T)\n",
        "        gpt.blocks[b].att.w_keys.weight = assign(gpt.blocks[b].att.w_keys.weight,k_w.T)\n",
        "        gpt.blocks[b].att.w_values.weight = assign(gpt.blocks[b].att.w_values.weight,v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split(params['blocks'][b]['attn']['c_attn']['b'],3,axis=-1)\n",
        "        gpt.blocks[b].att.w_queries.bias = assign(gpt.blocks[b].att.w_queries.bias,q_b)\n",
        "        gpt.blocks[b].att.w_keys.bias = assign(gpt.blocks[b].att.w_keys.bias,k_b)\n",
        "        gpt.blocks[b].att.w_values.bias = assign(gpt.blocks[b].att.w_values.bias,v_b)\n",
        "\n",
        "        gpt.blocks[b].att.out_layer.weight = assign(gpt.blocks[b].att.out_layer.weight, params['blocks'][b]['attn']['c_proj']['w'].T)\n",
        "        gpt.blocks[b].att.out_layer.bias = assign(gpt.blocks[b].att.out_layer.bias, params['blocks'][b]['attn']['c_proj']['b'])\n",
        "\n",
        "        gpt.blocks[b].ff.layers[0].weight = assign(gpt.blocks[b].ff.layers[0].weight, params['blocks'][b]['mlp']['c_fc']['w'].T)\n",
        "        gpt.blocks[b].ff.layers[0].bias = assign(gpt.blocks[b].ff.layers[0].bias, params['blocks'][b]['mlp']['c_fc']['b'])\n",
        "        gpt.blocks[b].ff.layers[2].weight = assign(gpt.blocks[b].ff.layers[2].weight, params['blocks'][b]['mlp']['c_proj']['w'].T)\n",
        "        gpt.blocks[b].ff.layers[2].bias = assign(gpt.blocks[b].ff.layers[2].bias, params['blocks'][b]['mlp']['c_proj']['b'])\n",
        "\n",
        "\n",
        "        gpt.blocks[b].norm1.scale = assign(gpt.blocks[b].norm1.scale, params['blocks'][b]['ln_1']['g'])\n",
        "        gpt.blocks[b].norm1.shift = assign(gpt.blocks[b].norm1.shift, params['blocks'][b]['ln_1']['b'])\n",
        "\n",
        "        gpt.blocks[b].norm2.scale = assign(gpt.blocks[b].norm2.scale, params['blocks'][b]['ln_2']['g'])\n",
        "        gpt.blocks[b].norm2.shift = assign(gpt.blocks[b].norm2.shift, params['blocks'][b]['ln_2']['b'])\n",
        "\n",
        "  gpt.final_norm.scale = assign(gpt.final_norm.scale,params['g'])\n",
        "  gpt.final_norm.shift = assign(gpt.final_norm.shift,params['b'])\n",
        "  gpt.out_head.weight = assign(gpt.out_head.weight, params['wte'])"
      ],
      "metadata": {
        "id": "MTweOadtGB_r"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_weights_into_gpt(gpt,params)\n",
        "gpt.to(device);"
      ],
      "metadata": {
        "id": "YOgywMD_QbIx"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "token_ids = generate(\n",
        "                     idx = text_to_token_ids(\"Every effort moves you\",tokenizer).to(device),\n",
        "                     context_length=1024,\n",
        "                     model=gpt,\n",
        "                     max_words = 20,\n",
        "                     temperature=1.5,top_k=50,end_token=None\n",
        "                     )"
      ],
      "metadata": {
        "id": "NxcEJFeNQhYA"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids_to_text(token_ids,tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "_cPRU81aTy1v",
        "outputId": "c55244ea-c153-41cd-861d-021ba13e32ee"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Every effort moves you toward an equal share for each vote plus half. Inequality is often not an accurate representation of human'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mN782XUkWSFe"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}