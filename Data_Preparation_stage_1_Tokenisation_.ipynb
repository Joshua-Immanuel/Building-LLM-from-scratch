{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2o5RlkZuQQN",
        "outputId": "3ea16059-9110-4898-ea05-ca16c207ef1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20406\n",
            "The verdict\n",
            "Edith wharton\n",
            "I had always thought Jack Gisburn rather a cheap genius--though a good fellow\n",
            "enough--so it was no great surprise to me to hear that, in the height of his glory, he\n",
            "had dropped his painting, married a rich widow, and established himself in a villa\n",
            "on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
            "\"The height of his glory\"--that was what the wo\n"
          ]
        }
      ],
      "source": [
        "with open(\"/content/theverdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
        "  raw_text= f.read()\n",
        "print(len(raw_text))\n",
        "print(raw_text[:400])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "MA5VEf63xebX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([,.:;\"()_!\\']|--|\\s)',raw_text)\n",
        "preprocessed = [ele for ele in preprocessed if ele.strip()]"
      ],
      "metadata": {
        "id": "75A9Z72RvTdd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Z71xShAxug3",
        "outputId": "fa840f25-ef23-417f-c676-243c91fa6c31"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " 'verdict',\n",
              " 'Edith',\n",
              " 'wharton',\n",
              " 'I',\n",
              " 'had',\n",
              " 'always',\n",
              " 'thought',\n",
              " 'Jack',\n",
              " 'Gisburn']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(preprocessed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zo1w-JwbyzPj",
        "outputId": "d8ffe3d3-aa6e-436f-f24b-d6a330992ca2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4642"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating token ID's"
      ],
      "metadata": {
        "id": "rJsdBlZByoUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size=len(all_words)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdHwOEUqxvx9",
        "outputId": "af1e171b-d8e9-4da1-8411-6307d418b105"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {value:count for count,value in enumerate(all_words)}"
      ],
      "metadata": {
        "id": "tVgMEU7aysy4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxHO9fsRzJSm",
        "outputId": "060740bd-f679-4447-8fd5-cf5dbf31ecae"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'!': 0,\n",
              " '\"': 1,\n",
              " \"'\": 2,\n",
              " '(': 3,\n",
              " ')': 4,\n",
              " ',': 5,\n",
              " '--': 6,\n",
              " '.': 7,\n",
              " ':': 8,\n",
              " ';': 9,\n",
              " '?': 10,\n",
              " 'A': 11,\n",
              " 'AM': 12,\n",
              " 'Ah': 13,\n",
              " 'Among': 14,\n",
              " 'And': 15,\n",
              " 'Are': 16,\n",
              " 'Arrt': 17,\n",
              " 'As': 18,\n",
              " 'At': 19,\n",
              " 'Be': 20,\n",
              " 'Begin': 21,\n",
              " 'Burlington': 22,\n",
              " 'But': 23,\n",
              " 'By': 24,\n",
              " 'Carlo': 25,\n",
              " 'Chicago': 26,\n",
              " 'Claude': 27,\n",
              " 'Come': 28,\n",
              " 'Croft': 29,\n",
              " 'Destroyed': 30,\n",
              " 'Devonshire': 31,\n",
              " 'Don': 32,\n",
              " 'Dubarry': 33,\n",
              " 'Edith': 34,\n",
              " 'Emperors': 35,\n",
              " 'FELT': 36,\n",
              " 'Florence': 37,\n",
              " 'For': 38,\n",
              " 'Gallery': 39,\n",
              " 'Gideon': 40,\n",
              " 'Gisburn': 41,\n",
              " 'Gisburns': 42,\n",
              " 'Grafton': 43,\n",
              " 'Greek': 44,\n",
              " 'Grindle': 45,\n",
              " 'Grindles': 46,\n",
              " 'HAD': 47,\n",
              " 'HAS': 48,\n",
              " 'HAVE': 49,\n",
              " 'Had': 50,\n",
              " 'Hang': 51,\n",
              " 'Has': 52,\n",
              " 'He': 53,\n",
              " 'Her': 54,\n",
              " 'Hermia': 55,\n",
              " 'His': 56,\n",
              " 'How': 57,\n",
              " 'I': 58,\n",
              " 'If': 59,\n",
              " 'In': 60,\n",
              " 'It': 61,\n",
              " 'Jack': 62,\n",
              " 'Jove': 63,\n",
              " 'Just': 64,\n",
              " 'KNOWN': 65,\n",
              " 'Lord': 66,\n",
              " 'MINE': 67,\n",
              " 'Made': 68,\n",
              " 'Miss': 69,\n",
              " 'Money': 70,\n",
              " 'Monte': 71,\n",
              " 'Moon-': 72,\n",
              " 'Mr': 73,\n",
              " 'Mrs': 74,\n",
              " 'My': 75,\n",
              " 'NEVER': 76,\n",
              " 'NOT': 77,\n",
              " 'Never': 78,\n",
              " 'No': 79,\n",
              " 'Now': 80,\n",
              " 'Nutley': 81,\n",
              " 'Of': 82,\n",
              " 'Oh': 83,\n",
              " 'On': 84,\n",
              " 'Once': 85,\n",
              " 'Only': 86,\n",
              " 'Or': 87,\n",
              " 'Perhaps': 88,\n",
              " 'Poor': 89,\n",
              " 'Professional': 90,\n",
              " 'RS': 91,\n",
              " 'Renaissance': 92,\n",
              " 'Rickham': 93,\n",
              " 'Riviera': 94,\n",
              " 'Rome': 95,\n",
              " 'Russian': 96,\n",
              " 'Sevres': 97,\n",
              " 'She': 98,\n",
              " 'Stroud': 99,\n",
              " 'Strouds': 100,\n",
              " 'Suddenly': 101,\n",
              " 'THAT': 102,\n",
              " 'THE': 103,\n",
              " 'That': 104,\n",
              " 'The': 105,\n",
              " 'Then': 106,\n",
              " 'There': 107,\n",
              " 'They': 108,\n",
              " 'This': 109,\n",
              " 'Those': 110,\n",
              " 'Though': 111,\n",
              " 'Thwing': 112,\n",
              " 'Thwings': 113,\n",
              " 'To': 114,\n",
              " 'Usually': 115,\n",
              " 'Venetian': 116,\n",
              " 'Victor': 117,\n",
              " 'WAS': 118,\n",
              " 'WERE': 119,\n",
              " 'Was': 120,\n",
              " 'We': 121,\n",
              " 'Well': 122,\n",
              " 'What': 123,\n",
              " 'When': 124,\n",
              " 'Why': 125,\n",
              " 'Yes': 126,\n",
              " 'You': 127,\n",
              " 'You?': 128,\n",
              " 'a': 129,\n",
              " 'abdication': 130,\n",
              " 'able': 131,\n",
              " 'about': 132,\n",
              " 'above': 133,\n",
              " 'abruptly': 134,\n",
              " 'absolute': 135,\n",
              " 'absorbed': 136,\n",
              " 'absurdity': 137,\n",
              " 'academic': 138,\n",
              " 'accuse': 139,\n",
              " 'accustomed': 140,\n",
              " 'across': 141,\n",
              " 'activity': 142,\n",
              " 'add': 143,\n",
              " 'added': 144,\n",
              " 'admirers': 145,\n",
              " 'adopted': 146,\n",
              " 'adulation': 147,\n",
              " 'advance': 148,\n",
              " 'aesthetic': 149,\n",
              " 'affect': 150,\n",
              " 'afraid': 151,\n",
              " 'after': 152,\n",
              " 'afterward': 153,\n",
              " 'again': 154,\n",
              " 'again?': 155,\n",
              " 'ago': 156,\n",
              " 'ah': 157,\n",
              " 'air': 158,\n",
              " 'alive': 159,\n",
              " 'all': 160,\n",
              " 'almost': 161,\n",
              " 'alone': 162,\n",
              " 'along': 163,\n",
              " 'always': 164,\n",
              " 'amazement': 165,\n",
              " 'amid': 166,\n",
              " 'among': 167,\n",
              " 'amplest': 168,\n",
              " 'amusing': 169,\n",
              " 'an': 170,\n",
              " 'and': 171,\n",
              " 'another': 172,\n",
              " 'answer': 173,\n",
              " 'answered': 174,\n",
              " 'any': 175,\n",
              " 'anything': 176,\n",
              " 'anywhere': 177,\n",
              " 'apparent': 178,\n",
              " 'apparently': 179,\n",
              " 'appearance': 180,\n",
              " 'appeared': 181,\n",
              " 'appointed': 182,\n",
              " 'are': 183,\n",
              " 'arm': 184,\n",
              " 'arm-chair': 185,\n",
              " 'arm-chairs': 186,\n",
              " 'arms': 187,\n",
              " 'art': 188,\n",
              " 'articles': 189,\n",
              " 'artist': 190,\n",
              " 'as': 191,\n",
              " 'aside': 192,\n",
              " 'asked': 193,\n",
              " 'at': 194,\n",
              " 'atmosphere': 195,\n",
              " 'atom': 196,\n",
              " 'attack': 197,\n",
              " 'attention': 198,\n",
              " 'attitude': 199,\n",
              " 'audacities': 200,\n",
              " 'away': 201,\n",
              " 'awful': 202,\n",
              " 'axioms': 203,\n",
              " 'azaleas': 204,\n",
              " 'back': 205,\n",
              " 'background': 206,\n",
              " 'balance': 207,\n",
              " 'balancing': 208,\n",
              " 'balustraded': 209,\n",
              " 'basking': 210,\n",
              " 'bath-rooms': 211,\n",
              " 'be': 212,\n",
              " 'beaming': 213,\n",
              " 'bean-stalk': 214,\n",
              " 'bear': 215,\n",
              " 'beard': 216,\n",
              " 'beauty': 217,\n",
              " 'became': 218,\n",
              " 'because': 219,\n",
              " 'becoming': 220,\n",
              " 'bed': 221,\n",
              " 'been': 222,\n",
              " 'before': 223,\n",
              " 'began': 224,\n",
              " 'begun': 225,\n",
              " 'behind': 226,\n",
              " 'being': 227,\n",
              " 'believed': 228,\n",
              " 'beneath': 229,\n",
              " 'bespoke': 230,\n",
              " 'better': 231,\n",
              " 'between': 232,\n",
              " 'big': 233,\n",
              " 'bits': 234,\n",
              " 'bitterness': 235,\n",
              " 'blocked': 236,\n",
              " 'born': 237,\n",
              " 'borne': 238,\n",
              " 'boudoir': 239,\n",
              " 'bravura': 240,\n",
              " 'break': 241,\n",
              " 'breaking': 242,\n",
              " 'breathing': 243,\n",
              " 'bric-a-brac': 244,\n",
              " 'briefly': 245,\n",
              " 'brings': 246,\n",
              " 'bronzes': 247,\n",
              " 'brought': 248,\n",
              " 'brown': 249,\n",
              " 'brush': 250,\n",
              " 'bull': 251,\n",
              " 'business': 252,\n",
              " 'but': 253,\n",
              " 'buying': 254,\n",
              " 'by': 255,\n",
              " 'called': 256,\n",
              " 'came': 257,\n",
              " 'can': 258,\n",
              " 'can?': 259,\n",
              " 'canvas': 260,\n",
              " 'canvases': 261,\n",
              " 'cards': 262,\n",
              " 'care': 263,\n",
              " 'career': 264,\n",
              " 'caught': 265,\n",
              " 'central': 266,\n",
              " 'chair': 267,\n",
              " 'chap': 268,\n",
              " 'characteristic': 269,\n",
              " 'charming': 270,\n",
              " 'cheap': 271,\n",
              " 'check': 272,\n",
              " 'cheeks': 273,\n",
              " 'chest': 274,\n",
              " 'chimney-piece': 275,\n",
              " 'chucked': 276,\n",
              " 'cigar': 277,\n",
              " 'cigarette': 278,\n",
              " 'cigars': 279,\n",
              " 'circulation': 280,\n",
              " 'circumstance': 281,\n",
              " 'circus-clown': 282,\n",
              " 'claimed': 283,\n",
              " 'clasping': 284,\n",
              " 'clear': 285,\n",
              " 'cleverer': 286,\n",
              " 'close': 287,\n",
              " 'clue': 288,\n",
              " 'coat': 289,\n",
              " 'collapsed': 290,\n",
              " 'colour': 291,\n",
              " 'come': 292,\n",
              " 'comfortable': 293,\n",
              " 'coming': 294,\n",
              " 'companion': 295,\n",
              " 'compared': 296,\n",
              " 'complex': 297,\n",
              " 'confident': 298,\n",
              " 'congesting': 299,\n",
              " 'conjugal': 300,\n",
              " 'constraint': 301,\n",
              " 'consummate': 302,\n",
              " 'contended': 303,\n",
              " 'continued': 304,\n",
              " 'corner': 305,\n",
              " 'corrected': 306,\n",
              " 'cotta': 307,\n",
              " 'could': 308,\n",
              " 'couldn': 309,\n",
              " 'count': 310,\n",
              " 'countenance': 311,\n",
              " 'couple': 312,\n",
              " 'course': 313,\n",
              " 'covered': 314,\n",
              " 'craft': 315,\n",
              " 'cried': 316,\n",
              " 'crossed': 317,\n",
              " 'crowned': 318,\n",
              " 'crumbled': 319,\n",
              " 'cry': 320,\n",
              " 'cured': 321,\n",
              " 'curiosity': 322,\n",
              " 'curious': 323,\n",
              " 'current': 324,\n",
              " 'curtains': 325,\n",
              " 'd': 326,\n",
              " 'dabble': 327,\n",
              " 'damask': 328,\n",
              " 'dancers': 329,\n",
              " 'dark': 330,\n",
              " 'dashed': 331,\n",
              " 'day': 332,\n",
              " 'days': 333,\n",
              " 'dead': 334,\n",
              " 'dead?': 335,\n",
              " 'deadening': 336,\n",
              " 'dear': 337,\n",
              " 'deep': 338,\n",
              " 'deerhound': 339,\n",
              " 'degree': 340,\n",
              " 'delicate': 341,\n",
              " 'demand': 342,\n",
              " 'denied': 343,\n",
              " 'deploring': 344,\n",
              " 'deprecating': 345,\n",
              " 'deprecatingly': 346,\n",
              " 'desire': 347,\n",
              " 'destroyed': 348,\n",
              " 'destruction': 349,\n",
              " 'desultory': 350,\n",
              " 'detail': 351,\n",
              " 'diagnosis': 352,\n",
              " 'did': 353,\n",
              " 'didn': 354,\n",
              " 'died': 355,\n",
              " 'dim': 356,\n",
              " 'dimmest': 357,\n",
              " 'dingy': 358,\n",
              " 'dining-room': 359,\n",
              " 'disarming': 360,\n",
              " 'discovery': 361,\n",
              " 'discrimination': 362,\n",
              " 'discussion': 363,\n",
              " 'disdain': 364,\n",
              " 'disdained': 365,\n",
              " 'disease': 366,\n",
              " 'disguised': 367,\n",
              " 'display': 368,\n",
              " 'dissatisfied': 369,\n",
              " 'distinguished': 370,\n",
              " 'distract': 371,\n",
              " 'divert': 372,\n",
              " 'do': 373,\n",
              " 'doesn': 374,\n",
              " 'doing': 375,\n",
              " 'domestic': 376,\n",
              " 'don': 377,\n",
              " 'done': 378,\n",
              " 'donkey': 379,\n",
              " 'down': 380,\n",
              " 'dozen': 381,\n",
              " 'dragged': 382,\n",
              " 'drawing-room': 383,\n",
              " 'drawing-rooms': 384,\n",
              " 'drawn': 385,\n",
              " 'dress-closets': 386,\n",
              " 'drew': 387,\n",
              " 'dropped': 388,\n",
              " 'each': 389,\n",
              " 'earth': 390,\n",
              " 'ease': 391,\n",
              " 'easel': 392,\n",
              " 'easy': 393,\n",
              " 'echoed': 394,\n",
              " 'economy': 395,\n",
              " 'effect': 396,\n",
              " 'effects': 397,\n",
              " 'efforts': 398,\n",
              " 'egregious': 399,\n",
              " 'eighteenth-century': 400,\n",
              " 'elbow': 401,\n",
              " 'elegant': 402,\n",
              " 'else': 403,\n",
              " 'embarrassed': 404,\n",
              " 'enabled': 405,\n",
              " 'end': 406,\n",
              " 'endless': 407,\n",
              " 'enjoy': 408,\n",
              " 'enlightenment': 409,\n",
              " 'enough': 410,\n",
              " 'ensuing': 411,\n",
              " 'equally': 412,\n",
              " 'equanimity': 413,\n",
              " 'escape': 414,\n",
              " 'established': 415,\n",
              " 'etching?': 416,\n",
              " 'even': 417,\n",
              " 'event': 418,\n",
              " 'ever': 419,\n",
              " 'everlasting': 420,\n",
              " 'every': 421,\n",
              " 'exasperated': 422,\n",
              " 'except': 423,\n",
              " 'excuse': 424,\n",
              " 'excusing': 425,\n",
              " 'existed': 426,\n",
              " 'expected': 427,\n",
              " 'exquisite': 428,\n",
              " 'exquisitely': 429,\n",
              " 'extenuation': 430,\n",
              " 'exterminating': 431,\n",
              " 'extracting': 432,\n",
              " 'eye': 433,\n",
              " 'eyebrows': 434,\n",
              " 'eyes': 435,\n",
              " 'face': 436,\n",
              " 'faces': 437,\n",
              " 'fact': 438,\n",
              " 'faded': 439,\n",
              " 'failed': 440,\n",
              " 'failure': 441,\n",
              " 'fair': 442,\n",
              " 'faith': 443,\n",
              " 'false': 444,\n",
              " 'familiar': 445,\n",
              " 'famille-verte': 446,\n",
              " 'fancy': 447,\n",
              " 'fashionable': 448,\n",
              " 'fate': 449,\n",
              " 'feather': 450,\n",
              " 'feet': 451,\n",
              " 'fell': 452,\n",
              " 'fellow': 453,\n",
              " 'felt': 454,\n",
              " 'few': 455,\n",
              " 'fewer': 456,\n",
              " 'finality': 457,\n",
              " 'find': 458,\n",
              " 'fingers': 459,\n",
              " 'first': 460,\n",
              " 'fit': 461,\n",
              " 'fitting': 462,\n",
              " 'five': 463,\n",
              " 'flash': 464,\n",
              " 'flashed': 465,\n",
              " 'florid': 466,\n",
              " 'flowers': 467,\n",
              " 'fluently': 468,\n",
              " 'flung': 469,\n",
              " 'follow': 470,\n",
              " 'followed': 471,\n",
              " 'fond': 472,\n",
              " 'footstep': 473,\n",
              " 'for': 474,\n",
              " 'forced': 475,\n",
              " 'forcing': 476,\n",
              " 'forehead': 477,\n",
              " 'foreign': 478,\n",
              " 'foreseen': 479,\n",
              " 'forgive': 480,\n",
              " 'forgotten': 481,\n",
              " 'form': 482,\n",
              " 'formed': 483,\n",
              " 'forming': 484,\n",
              " 'forward': 485,\n",
              " 'fostered': 486,\n",
              " 'found': 487,\n",
              " 'foundations': 488,\n",
              " 'four': 489,\n",
              " 'fragment': 490,\n",
              " 'fragments': 491,\n",
              " 'frame': 492,\n",
              " 'frames': 493,\n",
              " 'frequently': 494,\n",
              " 'friend': 495,\n",
              " 'from': 496,\n",
              " 'full': 497,\n",
              " 'fullest': 498,\n",
              " 'furiously': 499,\n",
              " 'furrowed': 500,\n",
              " 'garlanded': 501,\n",
              " 'garlands': 502,\n",
              " 'gave': 503,\n",
              " 'genial': 504,\n",
              " 'genius': 505,\n",
              " 'gesture': 506,\n",
              " 'get': 507,\n",
              " 'getting': 508,\n",
              " 'give': 509,\n",
              " 'given': 510,\n",
              " 'glad': 511,\n",
              " 'glanced': 512,\n",
              " 'glimpse': 513,\n",
              " 'gloried': 514,\n",
              " 'glory': 515,\n",
              " 'go': 516,\n",
              " 'going': 517,\n",
              " 'gone': 518,\n",
              " 'good': 519,\n",
              " 'good-': 520,\n",
              " 'good-breeding': 521,\n",
              " 'good-humoured': 522,\n",
              " 'got': 523,\n",
              " 'grace': 524,\n",
              " 'gradually': 525,\n",
              " 'gray': 526,\n",
              " 'grayish': 527,\n",
              " 'great': 528,\n",
              " 'greatest': 529,\n",
              " 'greatness': 530,\n",
              " 'grew': 531,\n",
              " 'groping': 532,\n",
              " 'growing': 533,\n",
              " 'had': 534,\n",
              " 'hadn': 535,\n",
              " 'hair': 536,\n",
              " 'half': 537,\n",
              " 'half-light': 538,\n",
              " 'half-mechanically': 539,\n",
              " 'hall': 540,\n",
              " 'hand': 541,\n",
              " 'hands': 542,\n",
              " 'handsome': 543,\n",
              " 'hanging': 544,\n",
              " 'happen': 545,\n",
              " 'happened': 546,\n",
              " 'happened?': 547,\n",
              " 'hard': 548,\n",
              " 'hardly': 549,\n",
              " 'have': 550,\n",
              " 'haven': 551,\n",
              " 'having': 552,\n",
              " 'he': 553,\n",
              " 'head': 554,\n",
              " 'hear': 555,\n",
              " 'heard': 556,\n",
              " 'heart': 557,\n",
              " 'height': 558,\n",
              " 'her': 559,\n",
              " 'here': 560,\n",
              " 'hermit': 561,\n",
              " 'herself': 562,\n",
              " 'hesitations': 563,\n",
              " 'hide': 564,\n",
              " 'high': 565,\n",
              " 'him': 566,\n",
              " 'himself': 567,\n",
              " 'hint': 568,\n",
              " 'his': 569,\n",
              " 'history': 570,\n",
              " 'history?': 571,\n",
              " 'holding': 572,\n",
              " 'home': 573,\n",
              " 'honour': 574,\n",
              " 'hooded': 575,\n",
              " 'hostess': 576,\n",
              " 'hot-house': 577,\n",
              " 'hour': 578,\n",
              " 'hours': 579,\n",
              " 'house': 580,\n",
              " 'how': 581,\n",
              " 'humoured': 582,\n",
              " 'hung': 583,\n",
              " 'husband': 584,\n",
              " 'idea': 585,\n",
              " 'idle': 586,\n",
              " 'idling': 587,\n",
              " 'if': 588,\n",
              " 'immediately': 589,\n",
              " 'in': 590,\n",
              " 'incense': 591,\n",
              " 'indifferent': 592,\n",
              " 'inevitable': 593,\n",
              " 'inevitably': 594,\n",
              " 'inflexible': 595,\n",
              " 'insensible': 596,\n",
              " 'insignificant': 597,\n",
              " 'instinctively': 598,\n",
              " 'instructive': 599,\n",
              " 'interesting': 600,\n",
              " 'into': 601,\n",
              " 'ironic': 602,\n",
              " 'irony': 603,\n",
              " 'irrelevance': 604,\n",
              " 'irrevocable': 605,\n",
              " 'is': 606,\n",
              " 'it': 607,\n",
              " 'it?': 608,\n",
              " 'its': 609,\n",
              " 'itself': 610,\n",
              " 'jardiniere': 611,\n",
              " 'jealousy': 612,\n",
              " 'jealousy?': 613,\n",
              " 'just': 614,\n",
              " 'keep': 615,\n",
              " 'kept': 616,\n",
              " 'kind': 617,\n",
              " 'knees': 618,\n",
              " 'knew': 619,\n",
              " 'knew?': 620,\n",
              " 'know': 621,\n",
              " 'laid': 622,\n",
              " 'lair': 623,\n",
              " 'landing': 624,\n",
              " 'language': 625,\n",
              " 'last': 626,\n",
              " 'late': 627,\n",
              " 'later': 628,\n",
              " 'latter': 629,\n",
              " 'laugh': 630,\n",
              " 'laughed': 631,\n",
              " 'lay': 632,\n",
              " 'leading': 633,\n",
              " 'lean': 634,\n",
              " 'learned': 635,\n",
              " 'least': 636,\n",
              " 'leathery': 637,\n",
              " 'leave': 638,\n",
              " 'led': 639,\n",
              " 'left': 640,\n",
              " 'leisure': 641,\n",
              " 'lends': 642,\n",
              " 'lent': 643,\n",
              " 'let': 644,\n",
              " 'lies': 645,\n",
              " 'life': 646,\n",
              " 'life-': 647,\n",
              " 'lift': 648,\n",
              " 'lifted': 649,\n",
              " 'light': 650,\n",
              " 'lightly': 651,\n",
              " 'like': 652,\n",
              " 'liked': 653,\n",
              " 'likeness': 654,\n",
              " 'line': 655,\n",
              " 'lines': 656,\n",
              " 'lingered': 657,\n",
              " 'lips': 658,\n",
              " 'lit': 659,\n",
              " 'little': 660,\n",
              " 'live': 661,\n",
              " 'll': 662,\n",
              " 'loathing': 663,\n",
              " 'long': 664,\n",
              " 'longed': 665,\n",
              " 'longer': 666,\n",
              " 'look': 667,\n",
              " 'looked': 668,\n",
              " 'looking': 669,\n",
              " 'lose': 670,\n",
              " 'loss': 671,\n",
              " 'lounging': 672,\n",
              " 'lovely': 673,\n",
              " 'lucky': 674,\n",
              " 'lump': 675,\n",
              " 'luncheon-table': 676,\n",
              " 'luxury': 677,\n",
              " 'lying': 678,\n",
              " 'made': 679,\n",
              " 'make': 680,\n",
              " 'man': 681,\n",
              " 'manage': 682,\n",
              " 'managed': 683,\n",
              " 'mantel-piece': 684,\n",
              " 'marble': 685,\n",
              " 'married': 686,\n",
              " 'may': 687,\n",
              " 'me': 688,\n",
              " 'meant': 689,\n",
              " 'mediocrity': 690,\n",
              " 'medium': 691,\n",
              " 'mentioned': 692,\n",
              " 'mere': 693,\n",
              " 'merely': 694,\n",
              " 'met': 695,\n",
              " 'might': 696,\n",
              " 'mighty': 697,\n",
              " 'millionaire': 698,\n",
              " 'mine': 699,\n",
              " 'minute': 700,\n",
              " 'minutes': 701,\n",
              " 'mirrors': 702,\n",
              " 'modest': 703,\n",
              " 'modesty': 704,\n",
              " 'moment': 705,\n",
              " 'money': 706,\n",
              " 'monumental': 707,\n",
              " 'mood': 708,\n",
              " 'morbidly': 709,\n",
              " 'more': 710,\n",
              " 'more?': 711,\n",
              " 'most': 712,\n",
              " 'mourn': 713,\n",
              " 'mourned': 714,\n",
              " 'moustache': 715,\n",
              " 'moved': 716,\n",
              " 'much': 717,\n",
              " 'muddling': 718,\n",
              " 'multiplied': 719,\n",
              " 'murmur': 720,\n",
              " 'muscles': 721,\n",
              " 'must': 722,\n",
              " 'my': 723,\n",
              " 'myself': 724,\n",
              " 'mysterious': 725,\n",
              " 'naive': 726,\n",
              " 'near': 727,\n",
              " 'nearly': 728,\n",
              " 'negatived': 729,\n",
              " 'nervous': 730,\n",
              " 'nervousness': 731,\n",
              " 'neutral': 732,\n",
              " 'never': 733,\n",
              " 'next': 734,\n",
              " 'no': 735,\n",
              " 'none': 736,\n",
              " 'not': 737,\n",
              " 'note': 738,\n",
              " 'nothing': 739,\n",
              " 'now': 740,\n",
              " 'nymphs': 741,\n",
              " 'oak': 742,\n",
              " 'obituary': 743,\n",
              " 'object': 744,\n",
              " 'objects': 745,\n",
              " 'occurred': 746,\n",
              " 'oddly': 747,\n",
              " 'of': 748,\n",
              " 'off': 749,\n",
              " 'off?': 750,\n",
              " 'often': 751,\n",
              " 'oh': 752,\n",
              " 'old': 753,\n",
              " 'on': 754,\n",
              " 'once': 755,\n",
              " 'one': 756,\n",
              " 'ones': 757,\n",
              " 'only': 758,\n",
              " 'onto': 759,\n",
              " 'open': 760,\n",
              " 'or': 761,\n",
              " 'other': 762,\n",
              " 'our': 763,\n",
              " 'ourselves': 764,\n",
              " 'out': 765,\n",
              " 'out?': 766,\n",
              " 'outline': 767,\n",
              " 'oval': 768,\n",
              " 'over': 769,\n",
              " 'own': 770,\n",
              " 'packed': 771,\n",
              " 'paid': 772,\n",
              " 'paint': 773,\n",
              " 'painted': 774,\n",
              " 'painter': 775,\n",
              " 'painting': 776,\n",
              " 'painting?': 777,\n",
              " 'pale': 778,\n",
              " 'paled': 779,\n",
              " 'palm-trees': 780,\n",
              " 'panel': 781,\n",
              " 'panelling': 782,\n",
              " 'pardonable': 783,\n",
              " 'pardoned': 784,\n",
              " 'part': 785,\n",
              " 'passages': 786,\n",
              " 'passing': 787,\n",
              " 'past': 788,\n",
              " 'pastels': 789,\n",
              " 'pathos': 790,\n",
              " 'patient': 791,\n",
              " 'people': 792,\n",
              " 'perceptible': 793,\n",
              " 'perfect': 794,\n",
              " 'persistence': 795,\n",
              " 'persuasively': 796,\n",
              " 'phrase': 797,\n",
              " 'picture': 798,\n",
              " 'pictures': 799,\n",
              " 'pictures?': 800,\n",
              " 'pines': 801,\n",
              " 'pink': 802,\n",
              " 'place': 803,\n",
              " 'placed': 804,\n",
              " 'plain': 805,\n",
              " 'platitudes': 806,\n",
              " 'pleased': 807,\n",
              " 'pockets': 808,\n",
              " 'point': 809,\n",
              " 'poised': 810,\n",
              " 'poor': 811,\n",
              " 'portrait': 812,\n",
              " 'posing': 813,\n",
              " 'possessed': 814,\n",
              " 'poverty': 815,\n",
              " 'predicted': 816,\n",
              " 'preliminary': 817,\n",
              " 'presenting': 818,\n",
              " 'presses': 819,\n",
              " 'prestidigitation': 820,\n",
              " 'pretty': 821,\n",
              " 'previous': 822,\n",
              " 'price': 823,\n",
              " 'pride': 824,\n",
              " 'princely': 825,\n",
              " 'prism': 826,\n",
              " 'problem': 827,\n",
              " 'proclaiming': 828,\n",
              " 'prodigious': 829,\n",
              " 'profusion': 830,\n",
              " 'protest': 831,\n",
              " 'prove': 832,\n",
              " 'public': 833,\n",
              " 'purblind': 834,\n",
              " 'purely': 835,\n",
              " 'pushed': 836,\n",
              " 'put': 837,\n",
              " 'qualities': 838,\n",
              " 'quality': 839,\n",
              " 'queerly': 840,\n",
              " 'question': 841,\n",
              " 'quickly': 842,\n",
              " 'quietly': 843,\n",
              " 'quite': 844,\n",
              " 'quote': 845,\n",
              " 'rain': 846,\n",
              " 'raised': 847,\n",
              " 'random': 848,\n",
              " 'rather': 849,\n",
              " 're': 850,\n",
              " 'real': 851,\n",
              " 'really': 852,\n",
              " 'reared': 853,\n",
              " 'reason': 854,\n",
              " 'reassurance': 855,\n",
              " 'recovering': 856,\n",
              " 'recreated': 857,\n",
              " 'reflected': 858,\n",
              " 'reflection': 859,\n",
              " 'regrets': 860,\n",
              " 'relatively': 861,\n",
              " 'remained': 862,\n",
              " 'remember': 863,\n",
              " 'reminded': 864,\n",
              " 'repeating': 865,\n",
              " 'represented': 866,\n",
              " 'reproduction': 867,\n",
              " 'resented': 868,\n",
              " 'resolve': 869,\n",
              " 'resources': 870,\n",
              " 'rest': 871,\n",
              " 'rich': 872,\n",
              " 'ridiculous': 873,\n",
              " 'robbed': 874,\n",
              " 'romantic': 875,\n",
              " 'room': 876,\n",
              " 'rose': 877,\n",
              " 'rule': 878,\n",
              " 'run': 879,\n",
              " 's': 880,\n",
              " 'said': 881,\n",
              " 'same': 882,\n",
              " 'satisfaction': 883,\n",
              " 'savour?': 884,\n",
              " 'saw': 885,\n",
              " 'say': 886,\n",
              " 'saying': 887,\n",
              " 'says': 888,\n",
              " 'scorn': 889,\n",
              " 'scornful': 890,\n",
              " 'secret': 891,\n",
              " 'secret?': 892,\n",
              " 'see': 893,\n",
              " 'seemed': 894,\n",
              " 'seen': 895,\n",
              " 'self-confident': 896,\n",
              " 'send': 897,\n",
              " 'sensation': 898,\n",
              " 'sensitive': 899,\n",
              " 'sent': 900,\n",
              " 'serious': 901,\n",
              " 'set': 902,\n",
              " 'sex': 903,\n",
              " 'shade': 904,\n",
              " 'shaking': 905,\n",
              " 'shall': 906,\n",
              " 'she': 907,\n",
              " 'shirked': 908,\n",
              " 'short': 909,\n",
              " 'should': 910,\n",
              " 'shoulder': 911,\n",
              " 'shoulders': 912,\n",
              " 'show': 913,\n",
              " 'showed': 914,\n",
              " 'showy': 915,\n",
              " 'shrug': 916,\n",
              " 'shrugged': 917,\n",
              " 'sight': 918,\n",
              " 'sign': 919,\n",
              " 'silent': 920,\n",
              " 'silver': 921,\n",
              " 'similar': 922,\n",
              " 'simpleton': 923,\n",
              " 'simplifications': 924,\n",
              " 'simply': 925,\n",
              " 'since': 926,\n",
              " 'single': 927,\n",
              " 'sitter': 928,\n",
              " 'sitters': 929,\n",
              " 'sketch': 930,\n",
              " 'skill': 931,\n",
              " 'slight': 932,\n",
              " 'slightly': 933,\n",
              " 'slowly': 934,\n",
              " 'small': 935,\n",
              " 'smile': 936,\n",
              " 'smiling': 937,\n",
              " 'sneer': 938,\n",
              " 'so': 939,\n",
              " 'solace': 940,\n",
              " 'some': 941,\n",
              " 'somebody': 942,\n",
              " 'something': 943,\n",
              " 'spacious': 944,\n",
              " 'spaniel': 945,\n",
              " 'speaking-tubes': 946,\n",
              " 'speculations': 947,\n",
              " 'spite': 948,\n",
              " 'splash': 949,\n",
              " 'square': 950,\n",
              " 'stairs': 951,\n",
              " 'stammer': 952,\n",
              " 'stand': 953,\n",
              " 'standing': 954,\n",
              " 'started': 955,\n",
              " 'stay': 956,\n",
              " 'still': 957,\n",
              " 'stocked': 958,\n",
              " 'stood': 959,\n",
              " 'stopped': 960,\n",
              " 'stopping': 961,\n",
              " 'straddling': 962,\n",
              " 'straight': 963,\n",
              " 'strain': 964,\n",
              " 'straining': 965,\n",
              " 'strange': 966,\n",
              " 'straw': 967,\n",
              " 'stream': 968,\n",
              " 'stroke': 969,\n",
              " 'strokes': 970,\n",
              " 'strolled': 971,\n",
              " 'strongest': 972,\n",
              " 'strongly': 973,\n",
              " 'struck': 974,\n",
              " 'studio': 975,\n",
              " 'stuff': 976,\n",
              " 'subject': 977,\n",
              " 'substantial': 978,\n",
              " 'suburban': 979,\n",
              " 'such': 980,\n",
              " 'suddenly': 981,\n",
              " 'suffered': 982,\n",
              " 'sugar': 983,\n",
              " 'suggested': 984,\n",
              " 'sunburn': 985,\n",
              " 'sunburnt': 986,\n",
              " 'sunlit': 987,\n",
              " 'superb': 988,\n",
              " 'sure': 989,\n",
              " 'surest': 990,\n",
              " 'surface': 991,\n",
              " 'surprise': 992,\n",
              " 'surprised': 993,\n",
              " 'surrounded': 994,\n",
              " 'suspected': 995,\n",
              " 'sweetly': 996,\n",
              " 'sweetness': 997,\n",
              " 'swelling': 998,\n",
              " 'swept': 999,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "   def __init__(self,vocab) -> None:\n",
        "      self.str_to_int = vocab\n",
        "      self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "   def encoder(self,text):\n",
        "    preprocessed = re.split(r'([,.:;\"()_!\\']|--|\\s)',text)\n",
        "    preprocessed = [ele for ele in preprocessed if ele.strip()]\n",
        "    ids= [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "   def decode(self,ids):\n",
        "    txt = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    txt= re.sub(r'\\s([,.:;\"()_!\\'])',r'\\1',txt)\n",
        "    return txt\n"
      ],
      "metadata": {
        "id": "jMNj07CezKIM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "txt =\"\"\"\n",
        "    Among his own sex fewer regrets were heard, and in his own\n",
        "    trade hardly a murmur\n",
        "    \"\"\"\n",
        "token_ids=tokenizer.encoder(txt)\n",
        "print(token_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7BUbP-E28Yt",
        "outputId": "bd9aeb4f-c207-4647-85bb-ea8c3887c7f0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14, 569, 770, 903, 456, 860, 1122, 556, 5, 171, 590, 569, 770, 1059, 549, 129, 720]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(token_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HD_MYLsd3W-C",
        "outputId": "a6c7d1cf-49fd-4220-f27d-7785acbcf4c1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Among his own sex fewer regrets were heard, and in his own trade hardly a murmur'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# what id the token is not in the vocab??\n",
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "txt =\"\"\"\n",
        "    Hello!!\n",
        "    \"\"\"\n",
        "token_ids=tokenizer.encoder(txt)\n",
        "print(token_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "cJ61ju203fpG",
        "outputId": "282695c2-2838-4066-b778-e1f9e9615151"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Hello'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-e730e612dc36>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mHello\u001b[0m\u001b[0;31m!\u001b[0m\u001b[0;31m!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtoken_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-5340fd785150>\u001b[0m in \u001b[0;36mencoder\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([,.:;\"()_!\\']|--|\\s)'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mele\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mele\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mele\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-5340fd785150>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'([,.:;\"()_!\\']|--|\\s)'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mele\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mele\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mele\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use special context tokens\n",
        "# like <|unk|> and <|endoftext|> when appending text from multiple sources\n",
        "all_words = sorted(set(preprocessed))\n",
        "all_words.extend(['<|endoftext|>','<|unk|>'])\n",
        "vocab = {value:count for count,value in enumerate(all_words)}"
      ],
      "metadata": {
        "id": "WVYmyB616yn8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZg_Xgqv7drU",
        "outputId": "8b72a4dd-8183-4b50-908e-d955d5d7af8f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1168"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "   def __init__(self,vocab) -> None:\n",
        "      self.str_to_int = vocab\n",
        "      self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "   def encoder(self,text):\n",
        "    preprocessed = re.split(r'([,.:;\"()_!?\\']|--|\\s)',text)\n",
        "    preprocessed = [ele for ele in preprocessed if ele.strip()]\n",
        "    preprocessed = [ele if ele in self.str_to_int else \"<|unk|>\" for ele in preprocessed]\n",
        "    ids= [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "   def decode(self,ids):\n",
        "    txt = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    txt= re.sub(r'\\s([,.:;\"()_?!\\'])',r'\\1',txt)\n",
        "    return txt\n"
      ],
      "metadata": {
        "id": "6JFg4w0U7js0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"hello! do you like tea?\"\n",
        "text2 = \"would you like to stay in my palace?\"\n",
        "final_text =\" <|endoftext|> \".join((text1,text2))\n",
        "print(final_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIn6iw3f8RmN",
        "outputId": "16f1a47b-0ae2-437f-9e4f-2156678c8808"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello! do you like tea? <|endoftext|> would you like to stay in my palace?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer2 = SimpleTokenizerV2(vocab)\n",
        "ids=tokenizer2.encoder(final_text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plXiJF6X9FuN",
        "outputId": "365ee0bd-dfa4-49ae-e31a-1862c30f17d1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1167, 0, 373, 1161, 652, 1006, 10, 1166, 1155, 1161, 652, 1048, 956, 590, 723, 1167, 10]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer2.decode(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3fACaA7h9S7k",
        "outputId": "c1973eb9-d598-4a51-9563-79fad0e08167"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|unk|>! do you like tea? <|endoftext|> would you like to stay in my <|unk|>?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BYTE PAIR ENCODING !!!"
      ],
      "metadata": {
        "id": "7H0gnHrphXHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOWmxxK5hYtr",
        "outputId": "dec778ce-16e9-42f7-961b-908c24aeca79"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken"
      ],
      "metadata": {
        "id": "_5z1G92SpO9P"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding('gpt2')"
      ],
      "metadata": {
        "id": "dnkDC6wSojpj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"hello will you have tea? <|endoftext|> what are you doing right now?   grabacoffee?\"\"\"\n",
        "ids=tokenizer.encode(text,allowed_special={\"<|endoftext|>\"})\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cv2LOl1_pGhA",
        "outputId": "1df0c4cc-5651-41e2-fd13-73236f5d006e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[31373, 481, 345, 423, 8887, 30, 220, 50256, 644, 389, 345, 1804, 826, 783, 30, 220, 220, 5552, 330, 2364, 1453, 30]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt= tokenizer.decode(ids)\n",
        "txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BzNfcP8HqJpu",
        "outputId": "39a4654e-dd46-4da7-e5c0-a66fde86a068"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello will you have tea? <|endoftext|> what are you doing right now?   grabacoffee?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"?\"\"\"\n",
        "ids=tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "our3NXlyqmxl",
        "outputId": "6de1aa3e-262a-48e9-f683-c7409f662637"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[19526, 254, 25001, 121, 171, 120, 234, 20015, 232, 25465, 25465, 36365, 242, 45250, 236, 20046, 230, 43718, 115, 30]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "txt=tokenizer.decode(ids)\n",
        "txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9o_JcTRgrv67",
        "outputId": "21117b32-836a-4794-d123-119471db8cd4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"  ?\"\"\"\n",
        "ids=tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEDufkcEr1S4",
        "outputId": "b9ba5b6f-92a1-4520-ecd7-b3bb2fb43c40"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[156, 108, 101, 156, 109, 222, 156, 108, 243, 156, 109, 223, 220, 156, 108, 236, 156, 108, 110, 156, 108, 122, 220, 156, 108, 231, 156, 108, 101, 156, 109, 235, 156, 108, 101, 156, 108, 122, 156, 108, 113, 156, 109, 223, 30]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zLTJISx15qsJ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset and Dataloader"
      ],
      "metadata": {
        "id": "Y28KbY0v5xHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch"
      ],
      "metadata": {
        "id": "0xNMl-ON5yk6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTDatasetV1(Dataset):\n",
        "      def __init__(self,txt,tokenizer,maxlength,stride):\n",
        "          self.input_ids=[]\n",
        "          self.output_ids=[]\n",
        "          ids= tokenizer.encode(txt,allowed_special={'<|endoftext|>'})\n",
        "          for i in range(0,len(ids)-maxlength,stride):\n",
        "              self.input_ids.append(torch.tensor(ids[i:i+maxlength]))\n",
        "              self.output_ids.append(torch.tensor(ids[i+1:i+maxlength+1]))\n",
        "\n",
        "      def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "      def __getitem__(self,idx):\n",
        "        return self.input_ids[idx],self.output_ids[idx]"
      ],
      "metadata": {
        "id": "159-47DX6ERE"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt,batch_size=4,maxlength=256,stride=128,shuffle=True,drop_last=True,num_workers=0):\n",
        "  tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
        "  dataset=GPTDatasetV1(txt,tokenizer,maxlength,stride)\n",
        "  dataloader= DataLoader(\n",
        "      dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=shuffle,\n",
        "      drop_last=drop_last,\n",
        "      num_workers=num_workers\n",
        "  )\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "sLlg8a-F9zde"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader= create_dataloader_v1(raw_text,batch_size=1,maxlength=4,stride=2,shuffle=False)\n",
        "data_iter= iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VjCp2Xt_vHc",
        "outputId": "f3fc530e-8baf-4714-cb7d-fe47426145b5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[  464, 15593,   198,  7407]]), tensor([[15593,   198,  7407,   342]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ry8DxhTBDif",
        "outputId": "a48e1c13-ca3d-46a8-ce32-0ab3f873c37d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[ 198, 7407,  342,  348]]), tensor([[ 7407,   342,   348, 41328]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader= create_dataloader_v1(raw_text,batch_size=8,maxlength=4,stride=2,shuffle=False)\n",
        "data_iter= iter(dataloader)\n",
        "input,target = next(data_iter)\n",
        "print(input)\n",
        "print(target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhx1eAo_BkDr",
        "outputId": "5c52bda9-026b-4e1d-89f4-06189ee057cd"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  464, 15593,   198,  7407],\n",
            "        [  198,  7407,   342,   348],\n",
            "        [  342,   348, 41328,   198],\n",
            "        [41328,   198,    40,   550],\n",
            "        [   40,   550,  1464,  1807],\n",
            "        [ 1464,  1807,  3619,   402],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [  271, 10899,  2138,   257]])\n",
            "tensor([[15593,   198,  7407,   342],\n",
            "        [ 7407,   342,   348, 41328],\n",
            "        [  348, 41328,   198,    40],\n",
            "        [  198,    40,   550,  1464],\n",
            "        [  550,  1464,  1807,  3619],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [  402,   271, 10899,  2138],\n",
            "        [10899,  2138,   257,  7026]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Token Embedding"
      ],
      "metadata": {
        "id": "GmfaLEviTEOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "TKzcz8x-Tst2"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=6\n",
        "num_dimensions=3\n",
        "torch.manual_seed(123)\n",
        "embedding_layer=torch.nn.Embedding(vocab_size,num_dimensions)"
      ],
      "metadata": {
        "id": "z3xo_1ZYTFyP"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k48AIF5TsDL",
        "outputId": "a7bf3c2f-0aa3-4302-d1de-f8357fbc9305"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 0.3374, -0.1778, -0.1690],\n",
              "        [ 0.9178,  1.5810,  1.3010],\n",
              "        [ 1.2753, -0.2010, -0.1606],\n",
              "        [-0.4015,  0.9666, -1.1481],\n",
              "        [-1.1589,  0.3255, -0.6315],\n",
              "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer(torch.tensor([1,4]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpnJxOSGUQ01",
        "outputId": "95ac2c71-a772-4a8c-fa4a-4c11338c04c6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.9178,  1.5810,  1.3010],\n",
              "        [-1.1589,  0.3255, -0.6315]], grad_fn=<EmbeddingBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pyt_jkihUXQQ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=50256\n",
        "num_dimensions=256\n",
        "torch.manual_seed(123)\n",
        "token_embedding_layer=torch.nn.Embedding(vocab_size,num_dimensions)"
      ],
      "metadata": {
        "id": "GMoedhjqVCHc"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings=token_embedding_layer(input)"
      ],
      "metadata": {
        "id": "nAre6CeIVUK0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLuPN0_IViLM",
        "outputId": "16236b48-8934-42d3-fd5c-572dbb4ef213"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 4, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_length= maxlength=4\n",
        "num_dimensions=256\n"
      ],
      "metadata": {
        "id": "RP7LzCypVkuZ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_encoding= torch.nn.Embedding(context_length,num_dimensions)"
      ],
      "metadata": {
        "id": "J9DzsnWwX2OC"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positional_encodings = pos_encoding(torch.arange(maxlength))"
      ],
      "metadata": {
        "id": "cFKsmovuYJeg"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positional_encodings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "furh9dFkYc7z",
        "outputId": "bf0cd33b-676b-495e-9f87-6f653d3dec7c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_embeddings = token_embeddings+positional_encodings"
      ],
      "metadata": {
        "id": "RDpzCXTFYndP"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlGpyh3IbcHY",
        "outputId": "7f9d3c49-bdb3-482d-94f4-e17f6128eb2a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 4, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qevnx2sXbdZV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}