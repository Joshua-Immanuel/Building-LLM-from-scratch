{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4x9GFM0xGOky"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GPT2_124M_CFG={\n",
        "    'dropout':0.1,\n",
        "    'n_layers':12,\n",
        "    'n_heads':12,\n",
        "    'emb_size':768,\n",
        "    'context_length':1024,\n",
        "    'vocab_size':50257\n",
        "}"
      ],
      "metadata": {
        "id": "V_XuWosyia1D"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# norm = x-mu/sqrt(var).         # to prevent interval covarinat shift and probide model stability while training\n",
        "# norm = scale * norm + shift\n",
        "class LayerNormalization(nn.Module):\n",
        "  def __init__(self,emb_size):\n",
        "       super().__init__()\n",
        "       self.scale = nn.Parameter(torch.ones(emb_size))\n",
        "       self.shift = nn.Parameter(torch.ones(emb_size))\n",
        "       self.eps = 1e-5\n",
        "\n",
        "  def forward(self,x):\n",
        "      mean = torch.mean(x,dim=-1,keepdim=True)\n",
        "      var = torch.var(x,dim=-1,keepdim=True,unbiased=False)\n",
        "      return self.scale* (x-mean)/torch.sqrt(var+self.eps) + self.shift"
      ],
      "metadata": {
        "id": "JDvcGvxHGiT2"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "  def forward(self,x):\n",
        "    return 0.5 * x *(1+ torch.tanh(torch.tensor((2/torch.pi)**0.5))*(x+0.044715*x**3))\n",
        "    # return 0.5*x*(1+torch.tanh(torch.tensor((2/torch.pi))**0.5)*(x+0.044715*x**3))"
      ],
      "metadata": {
        "id": "2Y6AlFsiZjA7"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(cfg['emb_size'],4*cfg['emb_size']),\n",
        "        GELU(),\n",
        "        nn.Linear(4*cfg['emb_size'],cfg['emb_size'])\n",
        "    )\n",
        "  def forward(self,x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "nBbEqO5-ZAUP"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self,din,dout,n_heads,context_length,dropout,qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.w_queries = nn.Linear(din,dout,qkv_bias)\n",
        "        self.w_keys = nn.Linear(din,dout,qkv_bias)\n",
        "        self.w_values = nn.Linear(din,dout,qkv_bias)\n",
        "        self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
        "        self.out_layer = nn.Linear(dout,dout)\n",
        "        self.n_heads= n_heads\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    batch, context_length, emb_size = x.shape\n",
        "    queries = self.w_queries(x)\n",
        "    keys = self.w_keys(x)\n",
        "    values = self.w_values(x)\n",
        "    head_dim = emb_size//self.n_heads\n",
        "\n",
        "    queries = queries.view(batch,context_length,self.n_heads,head_dim)\n",
        "    keys = keys.view(batch,context_length,self.n_heads,head_dim)\n",
        "    values = values.view(batch,context_length,self.n_heads,head_dim)\n",
        "     # b,cl,nheads,hd.\n",
        "\n",
        "\n",
        "     # b, nheads, cl, hd\n",
        "    queries = queries.transpose(1,2)\n",
        "    keys = keys.transpose(1,2)\n",
        "    values = values.transpose(1,2)\n",
        "\n",
        "    attention_scores = queries @ keys.transpose(2,3)\n",
        "       #b, nheads, cl,cl\n",
        "\n",
        "    attention_scores.masked_fill(self.mask.bool()[:context_length,:context_length],-torch.inf)\n",
        "\n",
        "    attention_weights = torch.softmax(attention_scores/(keys.shape[-1])**0.5, dim =-1)\n",
        "    attention_weights = self.dropout(attention_weights)\n",
        "    context_vectors = (attention_weights @ values).transpose(1,2)\n",
        "    # b, nheads,cl,cl.     b,nheads, cl, hd\n",
        "    # b, nheads, cl,hd.     .T -> b, cl,nheads,hd\n",
        "    context_vectors = context_vectors.contiguous().view(batch, context_length, emb_size)\n",
        "    context_vectors = self.out_layer(context_vectors)\n",
        "    return context_vectors"
      ],
      "metadata": {
        "id": "8DnJu2gKacdu"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.ff = FeedForward(cfg)\n",
        "    self.dropout = nn.Dropout(cfg['dropout'])\n",
        "    self.norm1 = LayerNormalization(cfg['emb_size'])\n",
        "    self.norm2 = LayerNormalization(cfg['emb_size'])\n",
        "    self.att = MultiheadAttention(\n",
        "        din = cfg['emb_size'],\n",
        "        dout = cfg['emb_size'],\n",
        "        n_heads=cfg['n_heads'],\n",
        "        context_length = cfg['context_length'],\n",
        "        dropout = cfg['dropout'])\n",
        "\n",
        "  def forward(self,x):\n",
        "    shortcut = x\n",
        "    x = self.norm1(x)\n",
        "    x= self.att(x)\n",
        "    x= self.dropout(x)\n",
        "    x= shortcut+x\n",
        "\n",
        "    shortcut = x\n",
        "    x = self.norm2(x)\n",
        "    x= self.ff(x)\n",
        "    x= self.dropout(x)\n",
        "    x= shortcut+x\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "ibjRx_PtgRFu"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input = torch.rand(2,3,768)\n",
        "# block= Transformer(GPT2_124M_CFG)\n",
        "# block.forward(input)"
      ],
      "metadata": {
        "id": "o5pppwNpuClc"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb = nn.Embedding(cfg['vocab_size'],cfg['emb_size'])\n",
        "    self.pos_emb = nn.Embedding(cfg['context_length'],cfg['emb_size'])\n",
        "    self.out_head = nn.Linear(cfg['emb_size'],cfg['vocab_size'],bias=False)\n",
        "    self.drop_emb = nn.Dropout(cfg['dropout'])\n",
        "    self.blocks = nn.Sequential(\n",
        "        *[Transformer(cfg) for _ in range(cfg['n_layers'])]\n",
        "    )\n",
        "    self.final_norm = LayerNormalization(cfg['emb_size'])\n",
        "\n",
        "  def forward(self,x):\n",
        "    batch, n_tokens = x.shape\n",
        "    token_embed = self.tok_emb(x)\n",
        "    position_embed = self.pos_emb(torch.arange(n_tokens,device=x.device))\n",
        "    x =  token_embed + position_embed\n",
        "    x= self.drop_emb(x)\n",
        "    x= self.blocks(x)\n",
        "    x= self.final_norm(x)\n",
        "    logits= self.out_head(x)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "ccagkS76jFSs"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "inputs = torch.tensor([\n",
        "    [6109,3626,6100,345],\n",
        "    [6109,1110,6622,257]\n",
        "])\n",
        "model = GPT2(GPT2_124M_CFG)\n",
        "logits = model.forward(inputs)\n",
        "print(logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uKt73DcjJn0",
        "outputId": "dc38b613-eded-4f1c-d04a-aa6feb9ec105"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-1.5969,  1.9405, -0.1171,  ...,  0.2547,  1.6430, -0.6668],\n",
            "         [-1.5396,  1.7169, -0.3379,  ...,  0.9980,  2.3826, -0.3360],\n",
            "         [-0.7847,  2.3319,  0.0410,  ...,  1.4021,  1.1724, -0.1535],\n",
            "         [-1.3134,  2.6708,  0.1153,  ...,  1.4573,  1.5512, -0.9585]],\n",
            "\n",
            "        [[-1.9150,  1.8283, -0.3165,  ...,  0.2451,  1.5907, -0.9045],\n",
            "         [-1.4808,  1.8762,  0.2355,  ...,  0.8987,  1.7394,  0.3615],\n",
            "         [-0.4588,  2.2907,  0.3171,  ...,  0.8931,  1.5476, -0.3596],\n",
            "         [-1.2323,  1.2671,  0.3746,  ...,  2.0974,  1.6325, -0.2108]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"{total_params:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JP_2J2Fr0NUS",
        "outputId": "1d9917ea-0b63-4421-9324-84ebe9871b9e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "163,009,536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_simple(idx,model,max_words,context_length):\n",
        "  model.eval()\n",
        "  for _ in range(max_words):\n",
        "   idx_new = idx[:,-context_length:]\n",
        "   with torch.no_grad():\n",
        "    logits = model(idx_new)\n",
        "   temp = logits[:,-1,:]\n",
        "   temp= torch.softmax(temp,dim=-1)\n",
        "   next = torch.argmax(temp,dim=-1,keepdim=True)\n",
        "   idx = torch.cat((idx,next),dim=-1)\n",
        "  return idx"
      ],
      "metadata": {
        "id": "x2daSTKksiMZ"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "inputs = torch.tensor([\n",
        "    [6109,3626,6100,345],\n",
        "    [6109,1110,6622,257]\n",
        "])\n",
        "next = get_text_simple(inputs,model,3,GPT2_124M_CFG['context_length'])\n",
        "print(next)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBgMsfSkz0xk",
        "outputId": "d075054d-7f03-4f99-cc6c-e88dc329c6c5"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 6109,  3626,  6100,   345,  1919, 21117, 12162],\n",
            "        [ 6109,  1110,  6622,   257, 41927,   343, 26097]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dYxM_9D805HJ"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GPT2_small_cl={\n",
        "    'dropout':0.1,\n",
        "    'n_layers':12,\n",
        "    'n_heads':12,\n",
        "    'emb_size':768,\n",
        "    'context_length':256,\n",
        "    'vocab_size':50257\n",
        "}"
      ],
      "metadata": {
        "id": "JU7LmweU0tgd"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input = torch.tensor([\n",
        "    [6109,  3626,  6100,   345],\n",
        "    [3626,  6100,   345, 120]\n",
        "])\n",
        "target = torch.tensor([\n",
        "    [3626,  6100,   345,120],\n",
        "    [6100,   345, 120, 460]\n",
        "])"
      ],
      "metadata": {
        "id": "W5EoBN3P0z8c"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits = model.forward(input)"
      ],
      "metadata": {
        "id": "c-7Y5J3N1l4E"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits.flatten(0,1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EgZqcpU1yor",
        "outputId": "79bacecb-b1ef-4a9c-8405-646f91746d95"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.9094,  1.8976, -0.5901,  ...,  0.2374,  1.3902, -0.7221],\n",
              "        [-1.2764,  1.6230, -0.2013,  ...,  1.0815,  2.3195,  0.1054],\n",
              "        [-0.9390,  2.4984, -0.2993,  ...,  1.4939,  1.3567,  0.1068],\n",
              "        ...,\n",
              "        [-1.1970,  2.2058, -0.6729,  ...,  1.5247,  1.4166,  0.3063],\n",
              "        [-1.1747,  2.5625,  0.0838,  ...,  0.7747,  1.6181, -0.9742],\n",
              "        [-1.3902,  1.6654,  0.0133,  ...,  1.6345,  1.8741, -0.4281]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXWGQQuY1tIF",
        "outputId": "671802ba-0f4c-4ab7-a254-51243dfcc3ef"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-1.9094,  1.8976, -0.5901,  ...,  0.2374,  1.3902, -0.7221],\n",
              "         [-1.2764,  1.6230, -0.2013,  ...,  1.0815,  2.3195,  0.1054],\n",
              "         [-0.9390,  2.4984, -0.2993,  ...,  1.4939,  1.3567,  0.1068],\n",
              "         [-1.0291,  2.5541,  0.0081,  ...,  1.6322,  1.5161, -0.8314]],\n",
              "\n",
              "        [[-1.3862,  1.6441, -0.3678,  ...,  0.6324,  2.0297, -0.3362],\n",
              "         [-1.1970,  2.2058, -0.6729,  ...,  1.5247,  1.4166,  0.3063],\n",
              "         [-1.1747,  2.5625,  0.0838,  ...,  0.7747,  1.6181, -0.9742],\n",
              "         [-1.3902,  1.6654,  0.0133,  ...,  1.6345,  1.8741, -0.4281]]],\n",
              "       grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = nn.functional.cross_entropy(logits.flatten(0,1),target.flatten())"
      ],
      "metadata": {
        "id": "hFgJ3Qo61gsb"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FEw6rn42ZJg",
        "outputId": "6ffd9272-5cb8-4bf9-9880-f9c047165966"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(11.3743, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity = torch.exp(loss)"
      ],
      "metadata": {
        "id": "Hr30j5Ld2Zwy"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3URad9-K2c3i",
        "outputId": "3f88946b-d0d4-4b6e-8198-6f137ec3ea1d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(87053.2422, grad_fn=<ExpBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/theverdict.txt',encoding='utf-8') as file:\n",
        "        txt_data = file.read()"
      ],
      "metadata": {
        "id": "T0roOVw42eF8"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "cWJtx4pVUZJI",
        "outputId": "17ec5568-449a-4635-cfd3-7db60be5fa88"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The verdict\\nEdith wharton\\nI had always thought Jack Gisburn rather a cheap genius--though a good fellow\\nenough--so it was no great surprise to me to hear that, in the height of his glory, he\\nhad dropped his painting, married a rich widow, and established himself in a villa\\non the Riviera. (Though I rather thought it would have been Rome or Florence.)\\n\"The height of his glory\"--that was what the women called it. I can hear Mrs.\\nGideon Thwing--his last Chicago sitter--deploring his unaccountable abdication.\\n\"Of course it\\'s going to send the value of my picture \\'way up; but I don\\'t think of\\nthat, Mr. Rickham--the loss to Arrt is all I think of.\" The word, on Mrs. Thwing\\'s\\nlips, multiplied its RS as though they were reflected in an endless vista of mirrors.\\nAnd it was not only the Mrs. Thwings who mourned. Had not the exquisite Hermia\\nCroft, at the last Grafton Gallery show, stopped me before Gisburn\\'s \"Moon-\\ndancers\" to say, with tears in her eyes: \"We shall not look upon its like again\"?\\nWell!--even through the prism of Hermia\\'s tears I felt able to face the fact with\\nequanimity. Poor Jack Gisburn! The women had made him--it was fitting that they\\nshould mourn him. Among his own sex fewer regrets were heard, and in his own\\ntrade hardly a murmur. Professional jealousy? Perhaps. If it were, the honour of the\\ncraft was vindicated by little Claude Nutley, who, in all good faith, brought out in\\nthe Burlington a very handsome \"obituary\" on Jack--one of those showy articles\\nstocked with random technicalities that I have heard (I won\\'t say by whom)\\ncompared to Gisburn\\'s painting. And so--his resolve being apparently irrevocable--\\nthe discussion gradually died out, and, as Mrs. Thwing had predicted, the price of\\n\"Gisburns\" went up.\\nIt was not till three years later that, in the course of a few weeks\\' idling on the\\nRiviera, it suddenly occurred to me to wonder why Gisburn had given up his\\npainting. On reflection, it really was a tempting problem. To accuse his wife would\\nhave been too easy--his fair sitters had been denied the solace of saying that Mrs.\\nGisburn had \"dragged him down.\" For Mrs. Gisburn--as such--had not existed till\\nnearly a year after Jack\\'s resolve had been taken. It might be that he had married\\nher-- since he liked his ease--because he didn\\'t want to go on painting; but it would\\nhave been hard to prove that he had given up his painting because he had married\\nher.\\nOf course, if she had not dragged him down, she had equally, as Miss Croft\\ncontended, failed to \"lift him up\"--she had not led him back to the easel. To put the\\nbrush into his hand again-- what a vocation for a wife! But Mrs. Gisburn appeared\\nto have disdained it--and I felt it might be interesting to find out why.\\nThe desultory life of the Riviera lends itself to such purely academic speculations;\\nand having, on my way to Monte Carlo, caught a glimpse of Jack\\'s balustraded\\nterraces between the pines, I had myself borne thither the next day.\\nI found the couple at tea beneath their palm-trees; and Mrs. Gisburn\\'s welcome was\\nso genial that, in the ensuing weeks, I claimed it frequently. It was not that my\\nhostess was \"interesting\": on that point I could have given Miss Croft the fullest\\nreassurance. It was just because she was NOT interesting--if I may be pardoned the\\nbull--that I found her so. For Jack, all his life, had been surrounded by interesting\\nwomen: they had fostered his art, it had been reared in the hot-house of their\\nadulation. And it was therefore instructive to note what effect the \"deadening\\natmosphere of mediocrity\" (I quote Miss Croft) was having on him.\\nI have mentioned that Mrs. Gisburn was rich; and it was immediately perceptible\\nthat her husband was extracting from this circumstance a delicate but substantial\\nsatisfaction. It is, as a rule, the people who scorn money who get most out of it;\\nand Jack\\'s elegant disdain of his wife\\'s big balance enabled him, with an\\nappearance of perfect good-breeding, to transmute it into objects of art and luxury.\\nTo the latter, I must add, he remained relatively indifferent; but he was buying\\nRenaissance bronzes and eighteenth-century pictures with a discrimination that\\nbespoke the amplest resources.\\n\"Money\\'s only excuse is to put beauty into circulation,\" was one of the axioms he\\nlaid down across the Sevres and silver of an exquisitely appointed luncheon-table,\\nwhen, on a later day, I had again run over from Monte Carlo; and Mrs. Gisburn,\\nbeaming on him, added for my enlightenment: \"Jack is so morbidly sensitive to\\nevery form of beauty.\"\\nPoor Jack! It had always been his fate to have women say such things of him: the\\nfact should be set down in extenuation. What struck me now was that, for the first\\ntime, he resented the tone. I had seen him, so often, basking under similar tributes--\\nwas it the conjugal note that robbed them of their savour? No--for, oddly enough, it\\nbecame apparent that he was fond of Mrs. Gisburn--fond enough not to see her\\nabsurdity. It was his own absurdity he seemed to be wincing under--his own\\nattitude as an object for garlands and incense.\\n\"My dear, since I\\'ve chucked painting people don\\'t say that stuff about me--they\\nsay it about Victor Grindle,\" was his only protest, as he rose from the table and\\nstrolled out onto the sunlit terrace.\\nI glanced after him, struck by his last word. Victor Grindle was, in fact, becoming\\nthe man of the moment--as Jack himself, one might put it, had been the man of the\\nhour. The younger artist was said to have formed himself at my friend\\'s feet, and I\\nwondered if a tinge of jealousy underlay the latter\\'s mysterious abdication. But no--\\nfor it was not till after that event that the rose Dubarry drawing-rooms had begun to\\ndisplay their \"Grindles.\"\\nI turned to Mrs. Gisburn, who had lingered to give a lump of sugar to her spaniel in\\nthe dining-room.\\n\"Why HAS he chucked painting?\" I asked abruptly.\\nShe raised her eyebrows with a hint of good-humoured surprise.\\n\"Oh, he doesn\\'t HAVE to now, you know; and I want him to enjoy himself,\" she\\nsaid quite simply.\\nI looked about the spacious white-panelled room, with its famille-verte vases\\nrepeating the tones of the pale damask curtains, and its eighteenth-century pastels\\nin delicate faded frames.\\n\"Has he chucked his pictures too? I haven\\'t seen a single one in the house.\"\\nA slight shade of constraint crossed Mrs. Gisburn\\'s open countenance. \"It\\'s his\\nridiculous modesty, you know. He says they\\'re not fit to have about; he\\'s sent them\\nall away except one--my portrait--and that I have to keep upstairs.\"\\nHis ridiculous modesty--Jack\\'s modesty about his pictures? My curiosity was\\ngrowing like the bean-stalk. I said persuasively to my hostess: \"I must really see\\nyour portrait, you know.\"\\nShe glanced out almost timorously at the terrace where her husband, lounging in a\\nhooded chair, had lit a cigar and drawn the Russian deerhound\\'s head between his\\nknees.\\n\"Well, come while he\\'s not looking,\" she said, with a laugh that tried to hide her\\nnervousness; and I followed her between the marble Emperors of the hall, and up\\nthe wide stairs with terra- cotta nymphs poised among flowers at each landing.\\nIn the dimmest corner of her boudoir, amid a profusion of delicate and\\ndistinguished objects, hung one of the familiar oval canvases, in the inevitable\\ngarlanded frame. The mere outline of the frame called up all Gisburn\\'s past!\\nMrs. Gisburn drew back the window-curtains, moved aside a jardiniere full of pink\\nazaleas, pushed an arm-chair away, and said: \"If you stand here you can just\\nmanage to see it. I had it over the mantel-piece, but he wouldn\\'t let it stay.\"\\nYes--I could just manage to see it--the first portrait of Jack\\'s I had ever had to\\nstrain my eyes over! Usually they had the place of honour--say the central panel in\\na pale yellow or rose Dubarry drawing-room, or a monumental easel placed so that\\nit took the light through curtains of old Venetian point. The more modest place\\nbecame the picture better; yet, as my eyes grew accustomed to the half-light, all the\\ncharacteristic qualities came out--all the hesitations disguised as audacities, the\\ntricks of prestidigitation by which, with such consummate skill, he managed to\\ndivert attention from the real business of the picture to some pretty irrelevance of\\ndetail. Mrs. Gisburn, presenting a neutral surface to work on--forming, as it were,\\nso inevitably the background of her own picture--had lent herself in an unusual\\ndegree to the display of this false virtuosity. The picture was one of Jack\\'s\\n\"strongest,\" as his admirers would have put it--it represented, on his part, a\\nswelling of muscles, a congesting of veins, a balancing, straddling and straining,\\nthat reminded one of the circus-clown\\'s ironic efforts to lift a feather. It met, in\\nshort, at every point the demand of lovely woman to be painted \"strongly\" because\\nshe was tired of being painted \"sweetly\"--and yet not to lose an atom of the\\nsweetness.\\n\"It\\'s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride. \"The\\nlast but one,\" she corrected herself-- \"but the other doesn\\'t count, because he\\ndestroyed it.\"\\n\"Destroyed it?\" I was about to follow up this clue when I heard a footstep and saw\\nJack himself on the threshold.\\nAs he stood there, his hands in the pockets of his velveteen coat, the thin brown\\nwaves of hair pushed back from his white forehead, his lean sunburnt cheeks\\nfurrowed by a smile that lifted the tips of a self-confident moustache, I felt to what\\na degree he had the same quality as his pictures--the quality of looking cleverer\\nthan he was.\\nHis wife glanced at him deprecatingly, but his eyes travelled past her to the\\nportrait.\\n\"Mr. Rickham wanted to see it,\" she began, as if excusing herself. He shrugged his\\nshoulders, still smiling.\\n\"Oh, Rickham found me out long ago,\" he said lightly; then, passing his arm\\nthrough mine: \"Come and see the rest of the house.\"\\nHe showed it to me with a kind of naive suburban pride: the bath-rooms, the\\nspeaking-tubes, the dress-closets, the trouser- presses--all the complex\\nsimplifications of the millionaire\\'s domestic economy. And whenever my wonder\\npaid the expected tribute he said, throwing out his chest a little: \"Yes, I really don\\'t\\nsee how people manage to live without that.\"\\nWell--it was just the end one might have foreseen for him. Only he was, through it\\nall and in spite of it all--as he had been through, and in spite of, his pictures--so\\nhandsome, so charming, so disarming, that one longed to cry out: \"Be dissatisfied\\nwith your leisure!\" as once one had longed to say: \"Be dissatisfied with your\\nwork!\"\\nBut, with the cry on my lips, my diagnosis suffered an unexpected check.\\n\"This is my own lair,\" he said, leading me into a dark plain room at the end of the\\nflorid vista. It was square and brown and leathery: no \"effects\"; no bric-a-brac,\\nnone of the air of posing for reproduction in a picture weekly--above all, no least\\nsign of ever having been used as a studio.\\nThe fact brought home to me the absolute finality of Jack\\'s break with his old life.\\n\"Don\\'t you ever dabble with paint any more?\" I asked, still looking about for a\\ntrace of such activity.\\n\"Never,\" he said briefly.\\n\"Or water-colour--or etching?\"\\nHis confident eyes grew dim, and his cheeks paled a little under their handsome\\nsunburn.\\n\"Never think of it, my dear fellow--any more than if I\\'d never touched a brush.\"\\nAnd his tone told me in a flash that he never thought of anything else.\\nI moved away, instinctively embarrassed by my unexpected discovery; and as I\\nturned, my eye fell on a small picture above the mantel-piece--the only object\\nbreaking the plain oak panelling of the room.\\n\"Oh, by Jove!\" I said.\\nIt was a sketch of a donkey--an old tired donkey, standing in the rain under a wall.\\n\"By Jove--a Stroud!\" I cried.\\nHe was silent; but I felt him close behind me, breathing a little quickly.\\n\"What a wonder! Made with a dozen lines--but on everlasting foundations. You\\nlucky chap, where did you get it?\"\\nHe answered slowly: \"Mrs. Stroud gave it to me.\"\\n\"Ah--I didn\\'t know you even knew the Strouds. He was such an inflexible hermit.\"\\n\"I didn\\'t--till after. . . . She sent for me to paint him when he was dead.\"\\n\"When he was dead? You?\"\\nI must have let a little too much amazement escape through my surprise, for he\\nanswered with a deprecating laugh: \"Yes--she\\'s an awful simpleton, you know,\\nMrs. Stroud. Her only idea was to have him done by a fashionable painter--ah,\\npoor Stroud! She thought it the surest way of proclaiming his greatness--of forcing\\nit on a purblind public. And at the moment I was THE fashionable painter.\"\\n\"Ah, poor Stroud--as you say. Was THAT his history?\"\\n\"That was his history. She believed in him, gloried in him--or thought she did. But\\nshe couldn\\'t bear not to have all the drawing-rooms with her. She couldn\\'t bear the\\nfact that, on varnishing days, one could always get near enough to see his pictures.\\nPoor woman! She\\'s just a fragment groping for other fragments. Stroud is the only\\nwhole I ever knew.\"\\n\"You ever knew? But you just said--\\n\"\\nGisburn had a curious smile in his eyes.\\n\"Oh, I knew him, and he knew me--only it happened after he was dead.\"\\nI dropped my voice instinctively. \"When she sent for you?\"\\n\"Yes--quite insensible to the irony. She wanted him vindicated-- and by me!\"\\nHe laughed again, and threw back his head to look up at the sketch of the donkey.\\n\"There were days when I couldn\\'t look at that thing--couldn\\'t face it. But I forced\\nmyself to put it here; and now it\\'s cured me--cured me. That\\'s the reason why I\\ndon\\'t dabble any more, my dear Rickham; or rather Stroud himself is the reason.\"\\nFor the first time my idle curiosity about my companion turned into a serious\\ndesire to understand him better.\\n\"I wish you\\'d tell me how it happened,\" I said.\\nHe stood looking up at the sketch, and twirling between his fingers a cigarette he\\nhad forgotten to light. Suddenly he turned toward me.\\n\"I\\'d rather like to tell you--because I\\'ve always suspected you of loathing my\\nwork.\"\\nI made a deprecating gesture, which he negatived with a good- humoured shrug.\\n\"Oh, I didn\\'t care a straw when I believed in myself--and now it\\'s an added tie\\nbetween us!\"\\nHe laughed slightly, without bitterness, and pushed one of the deep arm-chairs\\nforward. \"There: make yourself comfortable--and here are the cigars you like.\"\\nHe placed them at my elbow and continued to wander up and down the room,\\nstopping now and then beneath the picture.\\n\"How it happened? I can tell you in five minutes--and it didn\\'t take much longer to\\nhappen. . . . I can remember now how surprised and pleased I was when I got Mrs.\\nStroud\\'s note. Of course, deep down, I had always FELT there was no one like\\nhim-- only I had gone with the stream, echoed the usual platitudes about him, till I\\nhalf got to think he was a failure, one of the kind that are left behind. By Jove, and\\nhe WAS left behind-- because he had come to stay! The rest of us had to let\\nourselves be swept along or go under, but he was high above the current--on\\neverlasting foundations, as you say.\\n\"Well, I went off to the house in my most egregious mood--rather moved, Lord\\nforgive me, at the pathos of poor Stroud\\'s career of failure being crowned by the\\nglory of my painting him! Of course I meant to do the picture for nothing--I told\\nMrs. Stroud so when she began to stammer something about her poverty. I\\nremember getting off a prodigious phrase about the honour being MINE--oh, I was\\nprincely, my dear Rickham! I was posing to myself like one of my own sitters.\\n\"Then I was taken up and left alone with him. I had sent all my traps in advance,\\nand I had only to set up the easel and get to work. He had been dead only twenty-\\nfour hours, and he died suddenly, of heart disease, so that there had been no\\npreliminary work of destruction--his face was clear and untouched. I had met him\\nonce or twice, years before, and thought him insignificant and dingy. Now I saw\\nthat he was superb.\\n\"I was glad at first, with a merely aesthetic satisfaction: glad to have my hand on\\nsuch a \\'subject.\\' Then his strange life- likeness began to affect me queerly--as I\\nblocked the head in I felt as if he were watching me do it. The sensation was\\nfollowed by the thought: if he WERE watching me, what would he say to my way\\nof working? My strokes began to go a little wild--I felt nervous and uncertain.\\n\"Once, when I looked up, I seemed to see a smile behind his close grayish beard--\\nas if he had the secret, and were amusing himself by holding it back from me. That\\nexasperated me still more. The secret? Why, I had a secret worth twenty of his! I\\ndashed at the canvas furiously, and tried some of my bravura tricks. But they failed\\nme, they crumbled. I saw that he wasn\\'t watching the showy bits--I couldn\\'t distract\\nhis attention; he just kept his eyes on the hard passages between. Those were the\\nones I had always shirked, or covered up with some lying paint. And how he saw\\nthrough my lies!\\n\"I looked up again, and caught sight of that sketch of the donkey hanging on the\\nwall near his bed. His wife told me afterward it was the last thing he had done--just\\na note taken with a shaking hand, when he was down in Devonshire recovering\\nfrom a previous heart attack. Just a note! But it tells his whole history. There are\\nyears of patient scornful persistence in every line. A man who had swum with the\\ncurrent could never have learned that mighty up-stream stroke. . . .\\n\"I turned back to my work, and went on groping and muddling; then I looked at the\\ndonkey again. I saw that, when Stroud laid in the first stroke, he knew just what the\\nend would be. He had possessed his subject, absorbed it, recreated it. When had I\\ndone that with any of my things? They hadn\\'t been born of me--I had just adopted\\nthem. . . .\\n\"Hang it, Rickham, with that face watching me I couldn\\'t do another stroke. The\\nplain truth was, I didn\\'t know where to put it--I HAD NEVER KNOWN. Only,\\nwith my sitters and my public, a showy splash of colour covered up the fact--I just\\nthrew paint into their faces. . . . Well, paint was the one medium those dead eyes\\ncould see through--see straight to the tottering foundations underneath. Don\\'t you\\nknow how, in talking a foreign language, even fluently, one says half the time not\\nwhat one wants to but what one can? Well--that was the way I painted; and as he\\nlay there and watched me, the thing they called my \\'technique\\' collapsed like a\\nhouse of cards. He didn\\'t sneer, you understand, poor Stroud--he just lay there\\nquietly watching, and on his lips, through the gray beard, I seemed to hear the\\nquestion: \\'Are you sure you know where you\\'re coming out?\\'\\n\"If I could have painted that face, with that question on it, I should have done a\\ngreat thing. The next greatest thing was to see that I couldn\\'t--and that grace was\\ngiven me. But, oh, at that minute, Rickham, was there anything on earth I wouldn\\'t\\nhave given to have Stroud alive before me, and to hear him say: \\'It\\'s not too late--\\nI\\'ll show you how\\'?\\n\"It WAS too late--it would have been, even if he\\'d been alive. I packed up my\\ntraps, and went down and told Mrs. Stroud. Of course I didn\\'t tell her THAT--it\\nwould have been Greek to her. I simply said I couldn\\'t paint him, that I was too\\nmoved. She rather liked the idea--she\\'s so romantic! It was that that made her give\\nme the donkey. But she was terribly upset at not getting the portrait--she did so\\nwant him \\'done\\' by some one showy! At first I was afraid she wouldn\\'t let me off--\\nand at my wits\\' end I suggested Grindle. Yes, it was I who started Grindle: I told\\nMrs. Stroud he was the \\'coming\\' man, and she told somebody else, and so it got to\\nbe true. . . . And he painted Stroud without wincing; and she hung the picture\\namong her husband\\'s things. . . .\"\\nHe flung himself down in the arm-chair near mine, laid back his head, and clasping\\nhis arms beneath it, looked up at the picture above the chimney-piece.\\n\"I like to fancy that Stroud himself would have given it to me, if he\\'d been able to\\nsay what he thought that day.\"\\nAnd, in answer to a question I put half-mechanically--\"Begin again?\" he flashed\\nout. \"When the one thing that brings me anywhere near him is that I knew enough\\nto leave off?\"\\nHe stood up and laid his hand on my shoulder with a laugh. \"Only the irony of it is\\nthat I AM still painting--since Grindle\\'s doing it for me! The Strouds stand alone,\\nand happen once--but there\\'s no exterminating our kind of art.\"'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(txt_data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ChyMgLGdhO4",
        "outputId": "a6411d2b-2338-46ff-9bb7-90a38d7b142d"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The verdict\n",
            "Edith wharton\n",
            "I had always thought Jack Gisburn rather a cheap genius--though a good fel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tiktoken"
      ],
      "metadata": {
        "id": "74mpc9GkdyQZ"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken"
      ],
      "metadata": {
        "id": "II3iaaJPdnjh"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding('gpt2')"
      ],
      "metadata": {
        "id": "QV2O7dwsd0ch"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_tokens = tokenizer.encode(txt_data)"
      ],
      "metadata": {
        "id": "57Jy7moSd6tB"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(total_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eO1fNcmeDeU",
        "outputId": "1bf4a63e-04e6-4e51-90ed-45a91bb7e73a"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5314"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "  def __init__(self,txt,tokenizer,context_length,stride):\n",
        "      self.input_ids = []\n",
        "      self.target_ids = []\n",
        "\n",
        "      token_ids = tokenizer.encode(txt)\n",
        "      for i in range(0,len(token_ids)-context_length,stride):\n",
        "        self.input_ids.append(torch.tensor(token_ids[i:i+context_length]))\n",
        "        self.target_ids.append(torch.tensor(token_ids[i+1:i+context_length+1]))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "        return self.input_ids[idx],self.target_ids[idx]\n",
        "\n",
        "\n",
        "\n",
        "def create_dataloader_v1(txt,batch_size=4,max_length=256,stride=128,shuffle=True,drop_last=True,num_workers=0):\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "  dataset = GPTDatasetV1(txt,tokenizer,max_length,stride)\n",
        "  dataloader = DataLoader(\n",
        "      dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=shuffle,\n",
        "      drop_last=drop_last,\n",
        "      num_workers=num_workers\n",
        "  )\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "8CWcSQsKePRg"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = 0.9\n",
        "split_idx = int(train_ratio*len(txt_data))\n",
        "train_data = txt_data[:split_idx]\n",
        "valid_data = txt_data[split_idx+1:]\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "    txt=txt_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT2_small_cl['context_length'],\n",
        "    stride=GPT2_small_cl['context_length'],\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader_v1(\n",
        "    txt=valid_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT2_small_cl['context_length'],\n",
        "    stride=GPT2_small_cl['context_length'],\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=0\n",
        ")\n"
      ],
      "metadata": {
        "id": "6-_8yFKfg4WM"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x,y in train_loader:\n",
        "  print(x.shape,y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggaqpj6Ml3Pp",
        "outputId": "08a90747-7829-4289-e474-691142ee7fb7"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x,y in val_loader:\n",
        "  print(x.shape,y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxlUgJcsl6Qc",
        "outputId": "d5cc5837-8c54-4056-ef95-a0068c3ab676"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 256]) torch.Size([2, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch,target_batch,model,device):\n",
        "   input_batch,target_batch = input_batch.to(device),target_batch.to(device)\n",
        "   logits = model(input_batch)\n",
        "   return nn.functional.cross_entropy(logits.flatten(0,1),target_batch.flatten())\n",
        "\n",
        "def clac_class_loader(data_loader,model,device,num_batches = None):\n",
        "  total_loss =0.\n",
        "  if len(data_loader)==0:\n",
        "    return float(\"nan\")\n",
        "  elif num_batches is None:\n",
        "    num_batches = len(data_loader)\n",
        "  else:\n",
        "    num_batches = min(num_batches,len(data_loader))\n",
        "  for i,(input_batch,target_batch) in enumerate(data_loader):\n",
        "    if i<num_batches:\n",
        "      loss = calc_loss_batch(input_batch,target_batch,model,device)\n",
        "      total_loss += loss.item()\n",
        "    else:\n",
        "      break\n",
        "  return total_loss/num_batches\n",
        "\n"
      ],
      "metadata": {
        "id": "mu3OkPpkl8PQ"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "torch.manual_seed(123)\n",
        "with torch.no_grad():\n",
        "  train_loss = clac_class_loader(train_loader,model,device)\n",
        "  val_loss = clac_class_loader(val_loader,model,device)\n",
        "\n",
        "print(\"training loss:\",train_loss)\n",
        "print(\"valid loss:\",val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_SXzbvTptHM",
        "outputId": "eab92bba-6f3a-4901-ef46-fedfd97a0e50"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training loss: 11.054170894622803\n",
            "valid loss: 11.078924179077148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model,train_loader,val_loader,device,eval_iter):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    train_loss = clac_class_loader(train_loader,model,device,eval_iter)\n",
        "    val_loss = clac_class_loader(val_loader,model,device,eval_iter)\n",
        "  model.train()\n",
        "  return train_loss, val_loss"
      ],
      "metadata": {
        "id": "XnnX2L3ZSqfG"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0)\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "start_context = \"Every effort moves you\""
      ],
      "metadata": {
        "id": "oXj3N_NnWy6D"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_print_sample(model,tokenizer,device,start_context):\n",
        "  model.eval()\n",
        "  context_size = model.pos_emb.weight.shape[0]\n",
        "  encoded = text_to_token_ids(start_context,tokenizer).to(device)\n",
        "  with torch.no_grad():\n",
        "    token_ids = get_text_simple(idx=encoded,model=model,max_words=50,context_length=context_size)\n",
        "  decoded_text = token_ids_to_text(token_ids,tokenizer)\n",
        "  print(decoded_text.replace(\"\\n\",\" \"))\n",
        "  model.train()"
      ],
      "metadata": {
        "id": "OJ-kb4PMU7Xa"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_simple(model,train_loader,val_loader,\n",
        "                       optimizer,device,num_epochs,eval_freq,\n",
        "                       eval_iter,start_context,tokenizer):\n",
        "  train_losses, val_losses, track_tokens_seen = [],[],[]\n",
        "  tokens_seen, global_step=0,-1\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "      model.train()\n",
        "\n",
        "      for input_batch, target_batch in train_loader:\n",
        "          optimizer.zero_grad()\n",
        "          loss = calc_loss_batch(input_batch,target_batch,model,device)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          tokens_seen += input_batch.numel()\n",
        "          global_step += 1\n",
        "\n",
        "          if global_step % eval_freq == 0:\n",
        "              train_loss, val_loss = evaluate_model(\n",
        "                  model,train_loader,val_loader,device,eval_iter)\n",
        "              train_losses.append(train_loss)\n",
        "              val_losses.append(val_loss)\n",
        "              track_tokens_seen.append(tokens_seen)\n",
        "              print(f\"Epoch {epoch+1} (Step {global_step})\")\n",
        "              print(f\"Train loss:{train_loss} Valid loss:{val_loss}\")\n",
        "\n",
        "      generate_and_print_sample(\n",
        "            model,tokenizer,device,start_context\n",
        "      )\n",
        "  return train_losses, val_losses, track_tokens_seen\n",
        "\n",
        "# def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "#                        eval_freq, eval_iter, start_context, tokenizer):\n",
        "#     # Initialize lists to track losses and tokens seen\n",
        "#     train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "#     tokens_seen, global_step = 0, -1\n",
        "\n",
        "#     # Main training loop\n",
        "#     for epoch in range(num_epochs):\n",
        "#         model.train()  # Set model to training mode\n",
        "\n",
        "#         for input_batch, target_batch in train_loader:\n",
        "#             optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "#             loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "#             loss.backward() # Calculate loss gradients\n",
        "#             optimizer.step() # Update model weights using loss gradients\n",
        "#             tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
        "#             global_step += 1\n",
        "\n",
        "#             # Optional evaluation step\n",
        "#             if global_step % eval_freq == 0:\n",
        "#                 train_loss, val_loss = evaluate_model(\n",
        "#                     model, train_loader, val_loader, device, eval_iter)\n",
        "#                 train_losses.append(train_loss)\n",
        "#                 val_losses.append(val_loss)\n",
        "#                 track_tokens_seen.append(tokens_seen)\n",
        "#                 print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "#                       f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "#         # Print a sample text after each epoch\n",
        "#         generate_and_print_sample(\n",
        "#             model, tokenizer, device, start_context\n",
        "#         )\n",
        "\n",
        "#     return train_losses, val_losses, track_tokens_seen"
      ],
      "metadata": {
        "id": "G1ri_ujtqZnr"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "# torch.manual_seed(123)\n",
        "model =GPT2(GPT2_small_cl)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(),lr=0.0004, weight_decay=0.1)\n",
        "num_epochs=30\n",
        "train_losses, val_losses, track_tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "    eval_freq=5, eval_iter=5, start_context = \"Every effort moves you\",\n",
        "    tokenizer=tokenizer)\n",
        "end_time = time.time()\n",
        "print((end_time-start_time)/60)"
      ],
      "metadata": {
        "id": "ybWSo_U1ZYSl"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5s35y02auRe",
        "outputId": "5d63eb36-c8d3-4ae0-dde0-3076fb2a3b4d"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (Step 0)\n",
            "Train loss:4.629014682769776 Valid loss:5.365590572357178\n",
            "Epoch 1 (Step 5)\n",
            "Train loss:4.4085033416748045 Valid loss:5.4297943115234375\n",
            "Every effort moves you--and that he was the \"--and that he was \"--I had the \"--I had the \"I had been the \"I had been the \"I--I had been the \"I had been\n",
            "Epoch 2 (Step 10)\n",
            "Train loss:4.159240865707398 Valid loss:5.262295722961426\n",
            "Epoch 2 (Step 15)\n",
            "Train loss:3.6877690315246583 Valid loss:5.14303731918335\n",
            "Every effort moves you know. \"I had always of the \"Oh, and Mrs. \"--his, and \"I was \"I was \"Oh, and \"I was \"I didn't \"I was \"\n",
            "Epoch 3 (Step 20)\n",
            "Train loss:3.55321044921875 Valid loss:5.114974498748779\n",
            "Epoch 3 (Step 25)\n",
            "Train loss:3.329805374145508 Valid loss:5.184820652008057\n",
            "Every effort moves you know my way he was a little me--and I was that I had been to see the \"Oh, I felt \"I had been the \"I turned-rooms the \"I had been was not that I was \"\n",
            "Epoch 4 (Step 30)\n",
            "Train loss:2.9092750549316406 Valid loss:5.0409955978393555\n",
            "Epoch 4 (Step 35)\n",
            "Train loss:2.742257499694824 Valid loss:5.036468029022217\n",
            "Every effort moves you know I \"I had been too \"Oh, and and I \"I had been I had been, I \"-- \"I didn't know, and I had been \"Oh, and I had I \"Oh\n",
            "Epoch 5 (Step 40)\n",
            "Train loss:2.5694015502929686 Valid loss:4.854784965515137\n",
            "Epoch 5 (Step 45)\n",
            "Train loss:2.4084314346313476 Valid loss:4.753633975982666\n",
            "Every effort moves you know his pictures \"I didn't \"I looked with a little. \"I had always he was to the \"I had been to see it was to \"I must \"I didn't know you know, \"\n",
            "Epoch 6 (Step 50)\n",
            "Train loss:2.2049880981445313 Valid loss:4.595043659210205\n",
            "Epoch 6 (Step 55)\n",
            "Train loss:2.0692021369934084 Valid loss:4.646484375\n",
            "Every effort moves you know his \"I had always thought it, I was clear and untouched. \"I had always shir to the \"-- \"I H. \"Oh, I had a little it was about the \"I had always\n",
            "Epoch 7 (Step 60)\n",
            "Train loss:1.9674703359603882 Valid loss:4.6425604820251465\n",
            "Epoch 7 (Step 65)\n",
            "Train loss:1.7636841058731079 Valid loss:4.602457523345947\n",
            "Every effort moves you know; he was \"I turned; and I was that are left behind. \"Oh, and \"I looked about the \"I HAD NEVER KNOWN, I was \"Oh, at the picture was \"Oh\n",
            "Epoch 8 (Step 70)\n",
            "Train loss:1.6429723024368286 Valid loss:4.635207176208496\n",
            "Epoch 8 (Step 75)\n",
            "Train loss:1.4933988809585572 Valid loss:4.663055419921875\n",
            "Every effort moves you know, with a little. \"I didn't want him, and he was the \"Oh, and incense. \"My dear, on the \"Don't you ever dabble with paint any more?\" I \"Oh\n",
            "Epoch 9 (Step 80)\n",
            "Train loss:1.3599631547927857 Valid loss:4.58306360244751\n",
            "Epoch 9 (Step 85)\n",
            "Train loss:1.2575721502304078 Valid loss:4.627986431121826\n",
            "Every effort moves you know; he \"I didn't--his face was clear and untouched. \"The \"I didn't know where to put it--I HAD NEVER KNOWN. \"Ah--I didn't know you know.\" \"\n",
            "Epoch 10 (Step 90)\n",
            "Train loss:1.1026382803916932 Valid loss:4.696815490722656\n",
            "Epoch 10 (Step 95)\n",
            "Train loss:1.0197286009788513 Valid loss:4.606436252593994\n",
            "Every effort moves you know; he stood looking up at the sketch, one could always get near enough to see his pictures. \"Oh, and he turned toward me. \"I'd rather like to hear him say: 'It's not too late--I\n",
            "Epoch 11 (Step 100)\n",
            "Train loss:0.9272329092025757 Valid loss:4.6860857009887695\n",
            "Epoch 11 (Step 105)\n",
            "Train loss:0.9095544815063477 Valid loss:4.79958963394165\n",
            "Every effort moves you know; he was, and Mrs. \"--his own \"--I had met him, and \"I was \"My dear, paint. \"Don't you ever dabble with paint any more?\" I asked, and\n",
            "Epoch 12 (Step 110)\n",
            "Train loss:0.7686702489852906 Valid loss:4.708573818206787\n",
            "Epoch 12 (Step 115)\n",
            "Train loss:0.6446319818496704 Valid loss:4.745602607727051\n",
            "Every effort moves you. \"I dropped his eyes on the latter, I must add, the \"The \"Oh, all the \"-- \"but the other doesn't count, and I saw \"Oh, at the picture was fitting that they\n",
            "Epoch 13 (Step 120)\n",
            "Train loss:0.6022605895996094 Valid loss:4.755478858947754\n",
            "Epoch 13 (Step 125)\n",
            "Train loss:0.5034022092819214 Valid loss:4.901744842529297\n",
            "Every effort moves you know; he just kept his eyes on the latter, \"Oh, he remained relatively indifferent; but he was buying \"I turned toward me. \"I'd rather like the height of his glory, he \"I had a show\n",
            "Epoch 14 (Step 130)\n",
            "Train loss:0.4957026720046997 Valid loss:4.868312835693359\n",
            "Epoch 14 (Step 135)\n",
            "Train loss:0.47677122950553896 Valid loss:5.029932022094727\n",
            "Every effort moves you know; he just kept his eyes on the latter, I must add, he remained relatively indifferent; but he was buying \"I turned toward me. Gisburn! The rest of the millionaire's domestic economy. And whenever my wonder \"\n",
            "Epoch 15 (Step 140)\n",
            "Train loss:0.36308726072311404 Valid loss:5.076048851013184\n",
            "Epoch 15 (Step 145)\n",
            "Train loss:0.3387194395065308 Valid loss:5.054812431335449\n",
            "Every effort moves you know; he just kept his eyes on the hard passages between. Gisburn's an object for garlands and incense. \"My dear, since I've chucked painting people don't say that stuff about me--they say it\n",
            "Epoch 16 (Step 150)\n",
            "Train loss:0.2767952114343643 Valid loss:5.137451648712158\n",
            "Epoch 16 (Step 155)\n",
            "Train loss:0.29996553659439085 Valid loss:5.168461799621582\n",
            "Every effort moves you know; he just kept his eyes on the latter, I must add, he remained relatively indifferent; but he was buying \"Gisburn had \"dragged him down.\" For Mrs. Gisburn--as such--had not existed till\n",
            "Epoch 17 (Step 160)\n",
            "Train loss:0.331593906879425 Valid loss:5.343386650085449\n",
            "Epoch 17 (Step 165)\n",
            "Train loss:0.26292105615139005 Valid loss:5.2738037109375\n",
            "Every effort moves you know; he just kept his eyes on the hard passages between. Those were the plain truth was, and he WAS left behind-- because he had come to hear that, and to tell you--because I've always suspected you of loathing\n",
            "Epoch 18 (Step 170)\n",
            "Train loss:0.2325723499059677 Valid loss:5.181961536407471\n",
            "Epoch 18 (Step 175)\n",
            "Train loss:0.1983492076396942 Valid loss:5.337131023406982\n",
            "Every effort moves you know; he just kept his eyes on the hard passages between. Those were the plain truth was, I didn't know where to put it--all the complex \"Don't you ever dabble with paint any more?\" I asked, still\n",
            "Epoch 19 (Step 180)\n",
            "Train loss:0.1587187886238098 Valid loss:5.422092437744141\n",
            "Epoch 19 (Step 185)\n",
            "Train loss:0.1314476266503334 Valid loss:5.41598653793335\n",
            "Every effort moves you know; he just kept his eyes on the latter, I must add, he remained relatively indifferent; but he was buying Renaissance bronzes and eighteenth-century pictures with its famille-verte vases repeating the tones of\n",
            "Epoch 20 (Step 190)\n",
            "Train loss:0.1292269229888916 Valid loss:5.488914489746094\n",
            "Epoch 20 (Step 195)\n",
            "Train loss:0.1525908127427101 Valid loss:5.467348575592041\n",
            "Every effort moves you know; he just kept his eyes on the hard passages between. Those were the ones I had always shirked, years before, and thought him insignificant and dingy. Now I saw that he was superb. \"I was glad\n",
            "Epoch 21 (Step 200)\n",
            "Train loss:0.12780762314796448 Valid loss:5.462716579437256\n",
            "Epoch 21 (Step 205)\n",
            "Train loss:0.11065440624952316 Valid loss:5.664348602294922\n",
            "Every effort moves you know; he just kept his eyes on the latter, I must add, he remained relatively indifferent; but he was buying Renaissance bronzes and eighteenth-century pictures with a discrimination that that he was superb. \"I was glad\n",
            "Epoch 22 (Step 210)\n",
            "Train loss:0.09272196590900421 Valid loss:5.5821757316589355\n",
            "Epoch 22 (Step 215)\n",
            "Train loss:0.08646519407629967 Valid loss:5.599059581756592\n",
            "Every effort moves you know; he just kept his eyes on the latter, I must add, with a cigarette he had forgotten to light. characteristic qualities came out--all the ensuing weeks, I claimed it frequently. \"I looked up again, and\n",
            "Epoch 23 (Step 220)\n",
            "Train loss:0.0732825092971325 Valid loss:5.845229625701904\n",
            "Epoch 23 (Step 225)\n",
            "Train loss:0.07036950588226318 Valid loss:5.663710594177246\n",
            "Every effort moves you know; he just kept his eyes on the latter, I must add, he remained relatively indifferent; but he was buying Renaissance bronzes and eighteenth-century pictures with a discrimination that that he was superb. \"I was glad\n",
            "Epoch 24 (Step 230)\n",
            "Train loss:0.0623009167611599 Valid loss:5.72028112411499\n",
            "Epoch 24 (Step 235)\n",
            "Train loss:0.04859260320663452 Valid loss:5.744494915008545\n",
            "Every effort moves you know; he just kept his eyes on the hard passages between. Those were the ones I had always shirked, or covered up the wide stairs with terra- cotta nymphs poised among flowers at each landing. \"\n",
            "Epoch 25 (Step 240)\n",
            "Train loss:0.045430362969636914 Valid loss:5.838681697845459\n",
            "Epoch 25 (Step 245)\n",
            "Train loss:0.0422047458589077 Valid loss:5.645326137542725\n",
            "Every effort moves you know; he just kept his eyes on the latter, I must add, he remained relatively indifferent; but he was buying Renaissance bronzes and eighteenth-century pictures with its famille-verte vases repeating the tones of\n",
            "Epoch 26 (Step 250)\n",
            "Train loss:0.026116926595568656 Valid loss:5.846866607666016\n",
            "Epoch 26 (Step 255)\n",
            "Train loss:0.029681386053562166 Valid loss:5.883202075958252\n",
            "Every effort moves you know; he just kept his eyes on the latter, I must add, he remained relatively indifferent; but he was buying Renaissance bronzes and eighteenth-century pictures with a discrimination that that he was superb. \"I was glad\n",
            "Epoch 27 (Step 260)\n",
            "Train loss:0.027880362048745156 Valid loss:5.899194240570068\n",
            "Epoch 27 (Step 265)\n",
            "Train loss:0.03136728778481483 Valid loss:6.016967296600342\n",
            "Every effort moves you know; he just kept his eyes on the latter, I must add, he remained relatively indifferent; but he was buying Renaissance bronzes and eighteenth-century pictures with a discrimination that that he was superb. \"I was glad\n",
            "Epoch 28 (Step 270)\n",
            "Train loss:0.02670241966843605 Valid loss:5.8713250160217285\n",
            "Epoch 28 (Step 275)\n",
            "Train loss:0.019476760923862458 Valid loss:6.013524532318115\n",
            "Every effort moves you know; he just kept his eyes on the latter, I must add, he remained relatively indifferent; but he was buying Renaissance bronzes and eighteenth-century pictures with a discrimination that that he was superb. \"I was glad\n",
            "Epoch 29 (Step 280)\n",
            "Train loss:0.025227511674165724 Valid loss:5.946680068969727\n",
            "Epoch 29 (Step 285)\n",
            "Train loss:0.023000874742865562 Valid loss:6.092731952667236\n",
            "Every effort moves you know; he just kept his eyes on the latter, I must add, he remained relatively indifferent; but he was buying Renaissance bronzes and eighteenth-century pictures with a discrimination that bespoke the amplest resources. \"\n",
            "Epoch 30 (Step 290)\n",
            "Train loss:0.02428714483976364 Valid loss:6.063292026519775\n",
            "Epoch 30 (Step 295)\n",
            "Train loss:0.015820269100368024 Valid loss:5.91811466217041\n",
            "Every effort moves you know; he just kept his eyes on the latter, I must add, he remained relatively indifferent; but he was buying Renaissance bronzes and eighteenth-century pictures with its famille-verte vases repeating the tones of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pB3uhRHYgOIo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}