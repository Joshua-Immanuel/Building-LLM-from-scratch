{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Temperautre :- fancy term for dividing the logits by a value greater than 0.\n",
        "\n",
        "And multinomial probability distribution samples next token according to a probability score."
      ],
      "metadata": {
        "id": "wU0HZTnhD_ER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "ivUfTGwREfUT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FuxojacgD4Rj"
      },
      "outputs": [],
      "source": [
        "logits = torch.rand(9)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P98eJSwEekh",
        "outputId": "1aa50918-46ad-44c5-eb0a-b0f555786711"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.6658, 0.6068, 0.2864, 0.7993, 0.4788, 0.6706, 0.5592, 0.3304, 0.6525])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next_token_logits = torch.tensor(\n",
        "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
        ")"
      ],
      "metadata": {
        "id": "xCodQPuOEiRM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.argmax(next_token_logits,dim=-1,keepdim=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nrTNj3YFbhw",
        "outputId": "e6fce098-d04c-43d5-876d-1997ecc8968f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temperature1 =0.1\n",
        "temperature2 =1\n",
        "temperature3 =3\n",
        "probas1 = torch.round(torch.softmax(next_token_logits/temperature1,dim=-1)*1000)/1000\n",
        "probas2 = torch.round(torch.softmax(next_token_logits/temperature2,dim=-1)*1000)/1000\n",
        "probas3 = torch.round(torch.softmax(next_token_logits/temperature3,dim=-1)*1000)/1000\n",
        "print(probas1)\n",
        "print(probas2)\n",
        "print(probas3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2JAZ0ynFtxK",
        "outputId": "e774f026-2e3c-44ed-cf0a-a376ad9ad76d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0000, 0.0000, 0.0000, 0.9910, 0.0000, 0.0000, 0.0000, 0.0090, 0.0000])\n",
            "tensor([0.0610, 0.0020, 0.0000, 0.5720, 0.0030, 0.0000, 0.0000, 0.3580, 0.0040])\n",
            "tensor([0.1570, 0.0470, 0.0190, 0.3310, 0.0600, 0.0200, 0.0190, 0.2830, 0.0630])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.multinomial(probas1,num_samples=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7A0xTUb9GDIw",
        "outputId": "c06b09d2-5d09-4a7c-8138-79727bd92037"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.multinomial(probas2,num_samples=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3tPXSKvHD6J",
        "outputId": "05388cda-1021-4374-d851-74e28a9e1fab"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([7])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.multinomial(probas3,num_samples=1).item()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IC5eDhDEHHoS",
        "outputId": "22f11299-c203-4a10-dda2-b673a2a0ee02"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Top-K Sampling"
      ],
      "metadata": {
        "id": "qNrvvhsnnNwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "next_token_logits = torch.tensor(\n",
        "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
        ")"
      ],
      "metadata": {
        "id": "GIAx9rpRHL_3"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topk=3\n",
        "values,indices = torch.topk(next_token_logits,topk)\n",
        "print(values,indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awCbsW7YnV96",
        "outputId": "0956782c-8683-4f6b-99c5-5953aabf8824"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([6.7500, 6.2800, 4.5100]) tensor([3, 7, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_logits = torch.where(\n",
        "    condition=next_token_logits < values[-1],\n",
        "    input = torch.tensor(float(\"-inf\")),\n",
        "    other= next_token_logits\n",
        ")\n",
        "new_logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PrQKbPKnqGv",
        "outputId": "970f2ed7-7d77-4c42-a495-c44583f3145d"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_logits = torch.softmax(new_logits,dim=-1)\n",
        "new_logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHoveqEqoslr",
        "outputId": "bfb03492-41f8-4c40-d21e-2b971a427f79"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(idx,context_length,model,max_tokens,temperature=0.0,top_k=None,end_token=None):\n",
        "  for _ in range(max_tokens):\n",
        "      new_idx = idx[:,-context_length:]\n",
        "      with torch.no_grad():\n",
        "        logits = model(new_idx)\n",
        "      logits = logits[:,-1,:]\n",
        "\n",
        "      #temperature scaling + topk sampling\n",
        "      if top_k is not None:\n",
        "          new_logits_values,new_logits_idx = torch.topk(logits,top_k)\n",
        "          logits = torch.where(condition= logits<new_logits_values[:,-1],input = torch.tensor(float(\"-inf\")),other= logits)\n",
        "      if temperature>0.0:\n",
        "          logits = logits/temperature\n",
        "          probas = torch.softmax(logits,dim=-1)\n",
        "          next_idx = torch.multinomial(probas,num_samples=1)\n",
        "      else:\n",
        "          next_idx = torch.argmax(logits,dim=-1,keep_dim=True)\n",
        "      if next_idx==end_token:\n",
        "          break\n",
        "  idx= torch.cat((idx,next_idx),dim=-1)\n",
        "  return idx\n"
      ],
      "metadata": {
        "id": "PhfwZf6sov7I"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(idx,context_length,model,max_words,temperature=0.0,top_k=None,end_token=None):\n",
        "  model.eval()\n",
        "  for _ in range(max_words):\n",
        "      new_idx = idx[:,-context_length:]\n",
        "      with torch.no_grad():\n",
        "        logits = model(new_idx)\n",
        "      logits = logits[:,-1,:]\n",
        "\n",
        "     # temperature scaling + topk sampling\n",
        "      if top_k is not None:\n",
        "          top_logits,_ = torch.topk(logits,top_k)\n",
        "          min_val = top_logits[:,-1].unsqueeze(-1)\n",
        "          logits = torch.where(logits<min_val,torch.tensor(float(\"-inf\")).to(logits.device),logits)\n",
        "      if temperature>0.0:\n",
        "          logits = logits/temperature\n",
        "          probas = torch.softmax(logits,dim=-1)\n",
        "          next = torch.multinomial(probas,num_samples=1)\n",
        "      else:\n",
        "        logits = torch.softmax(logits,dim=-1)\n",
        "        next = torch.argmax(logits,dim=-1,keepdim=True)\n",
        "      if next==end_token:\n",
        "          break\n",
        "      idx = torch.cat((idx,next),dim=-1)\n",
        "  return idx\n"
      ],
      "metadata": {
        "id": "o88wBq3s2XO1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}