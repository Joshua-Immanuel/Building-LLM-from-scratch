{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ISUL3ltRyWum"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M={\n",
        "    \"vocab_size\":50257,\n",
        "    \"context_length\": 1024,\n",
        "    \"emb_dim\":768,\n",
        "    \"n_head\":12,\n",
        "    \"n_layers\":12,\n",
        "    \"drop_rate\":0.5,\n",
        "    \"qkv_bias\":False\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch"
      ],
      "metadata": {
        "id": "rCf3ttLkzoKF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer_Norm(nn.Module):\n",
        "  def __init__(self,emb_dim):\n",
        "    super().__init__()\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.eps = 1e-5\n",
        "\n",
        "  def forward(self,x):\n",
        "    mean = x.mean(dim=-1,keepdim=True)\n",
        "    var = x.var(dim=-1,keepdim=True, unbiased=True)\n",
        "    return self.scale*(x-mean)/torch.sqrt(var+self.eps) + self.shift\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self,x):\n",
        "    return 0.5*x*(1+torch.tanh(torch.tensor((2/torch.pi))**0.5)*(x+0.044715*x**3))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "        super().__init__()\n",
        "        self.layers =  nn.Sequential(\n",
        "        nn.Linear(cfg['emb_dim'],4*cfg['emb_dim']),    #expansion\n",
        "        GELU(),   #non linear activation\n",
        "        nn.Linear(4*cfg['emb_dim'],cfg['emb_dim'])  #compression\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "bDzZVAFCzg6v"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self,din,dout,context_length,dropout,num_heads,qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.w_queries = nn.Linear(din,dout,qkv_bias)\n",
        "    self.w_keys = nn.Linear(din,dout,qkv_bias)\n",
        "    self.w_values = nn.Linear(din,dout,qkv_bias)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    self.context_length=context_length\n",
        "    self.register_buffer('mask',torch.triu(torch.ones(context_length,context_length)))\n",
        "    self.num_heads=num_heads\n",
        "\n",
        "  def forward(self,x):\n",
        "    b,contextlength, emd_size = x.shape\n",
        "    num_heads=self.num_heads\n",
        "    head_dim = emd_size//num_heads\n",
        "\n",
        "    queries = self.w_queries(x)\n",
        "    keys = self.w_keys(x)\n",
        "    values = self.w_values(x)\n",
        "\n",
        "    queries = queries.view(b,contextlength,num_heads,head_dim)\n",
        "    keys = queries.view(b,contextlength,num_heads,head_dim)\n",
        "    values = queries.view(b,contextlength,num_heads,head_dim)\n",
        "\n",
        "    queries = queries.transpose(1,2)\n",
        "    keys = keys.transpose(1,2)\n",
        "    values = values.transpose(1,2)\n",
        "\n",
        "    attention_scores = queries @ keys.transpose(2,3)\n",
        "    attention_scores.masked_fill(self.mask.bool()[:contextlength, :contextlength],-torch.inf)\n",
        "    attention_weights = torch.softmax(attention_scores/num_heads**0.5,dim=-1)\n",
        "    attention_weights = self.dropout(attention_weights)\n",
        "    context_vectors = (attention_weights @ values).transpose(1,2)\n",
        "    context_vectors = context_vectors.contiguous().view(b,contextlength,emd_size)\n",
        "    return context_vectors\n"
      ],
      "metadata": {
        "id": "zXbuduqI-Y_w"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "\n",
        "  def __init__(self,cfg):\n",
        "      super().__init__()\n",
        "      self.norm1 = Layer_Norm(cfg['emb_dim'])\n",
        "      self.norm2 = Layer_Norm(cfg['emb_dim'])\n",
        "      self.att = MultiheadAttention(din=cfg['emb_dim'],\n",
        "                                    dout=cfg['emb_dim'],\n",
        "                                    context_length=cfg['context_length'],\n",
        "                                    dropout=cfg['drop_rate'],\n",
        "                                    num_heads=cfg['n_head'])\n",
        "      self.ff = FeedForward(cfg)\n",
        "      self.drop_shortcut = nn.Dropout(cfg['drop_rate'])\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "      shortcut = x\n",
        "      x = self.norm1(x)\n",
        "      x = self.att(x)\n",
        "      x = self.drop_shortcut(x)\n",
        "      x = x+shortcut\n",
        "      shortcut = x\n",
        "      x = self.norm2(x)\n",
        "      x = self.ff(x)\n",
        "      x = self.drop_shortcut(x)\n",
        "      x = x+shortcut\n",
        "\n",
        "      return x\n"
      ],
      "metadata": {
        "id": "FNU_aWIE23Zv"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "x = torch.rand(1,2,768)\n",
        "block = Transformer(GPT_CONFIG_124M)\n",
        "op=block.forward(x)\n",
        "print(op.shape)\n",
        "print(op)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5bFX77pHcX0",
        "outputId": "a6a6cc1d-7eb6-48c3-aaac-82e2f5a25869"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 2, 768])\n",
            "tensor([[[-0.4187,  0.5166,  0.2517,  ...,  0.9541,  0.8567,  0.6279],\n",
            "         [-0.3046,  0.4029,  0.3019,  ..., -0.1490,  0.6203,  0.7598]]],\n",
            "       grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zf3KVHelKyok"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}